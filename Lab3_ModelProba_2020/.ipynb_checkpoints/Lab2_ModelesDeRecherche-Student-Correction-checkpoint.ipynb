{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QICsOhm7j_uP"
   },
   "source": [
    "# Recherche d'Information et traitement de données massives\n",
    "## Lab 2 : Modèles de Recherche\n",
    "\n",
    "L'objectif de cette séance est de mettre en oeuvre les différents modèles de recherche vus en cours. La première partie du Lab correspond à des exercices d'application de cours sur les différents modèles. La deuxième partie est la mise en oeuvre des différents modèles sur le corpus TIME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSc9RaPWNzOF"
   },
   "source": [
    "\n",
    "## Partie 1 : Exercices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov68hUwmNzOH"
   },
   "source": [
    "### MODELE BOOLEEN\n",
    "\n",
    "Dans cet exercice, on considère une collection très petite constituée des documents ci-dessous :\n",
    "\n",
    "1. Doc1 = \"information retrieval and massive data processing\"\n",
    "2. Doc2 = \"introduction to information retrieval \"\n",
    "3. Doc3 = \"mining massive dataset\"\n",
    "4. Doc4 = \"modern information retrieval\"\n",
    "5. Doc5 = \"search engine information retrieval in practice\"\n",
    "6. Doc6 = \"information retrieval implementing and evaluating search engine\"\n",
    "\n",
    "**Exercice 1**\n",
    "\n",
    "Construire à la main la matrice terme-incidence de cette collection en considérant l'ensemble de mots vides suivants `Stop_words = [\"and\", \"to\", \"in\"]`.\n",
    "\n",
    "\n",
    "<img src=\"./Figures/matriceincidence.png\" width=\"500\" height=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bia9wilfj_uT"
   },
   "source": [
    "**Exercice 2**\n",
    "\n",
    "Quels sont les réponses aux requêtes ci-dessous :\n",
    "\n",
    "1. Query 1 = \"information AND retrieval AND NOT massive\"\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "**110111 AND 110111 AND 010111 = 010111 soit l'ensemble [DOC_2,DOC_4, DOC_5,DOC_6]**\n",
    "\n",
    "\n",
    "2. Query 2 = \"search AND engine AND NOT practice \"\n",
    "\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "**000011 AND 000011 AND 111101 =000001 soit l'ensemble [DOC_6]**\n",
    "\n",
    "\n",
    "3. Query 3 = \"(information OR search) AND retrieval\"\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "**(110111 OR 000011) AND 110111 = 110111 AND 110111 soit l'ensemble [DOC_1,DOC_2,DOC_4, DOC_5,DOC_6]**\n",
    "\n",
    "\n",
    "\n",
    "**Exercice 3 (OPTIONNEL)**\n",
    "\n",
    "Nous allons maintenant vérifier les réponses aux exercices 1 et 2 de manière expérimentale. On considère donc la collection comme représentée ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqYfaWF-j_uV"
   },
   "outputs": [],
   "source": [
    "D1 = \"information retrieval and massive data processing\"\n",
    "D2 = \"introduction to information retrieval\"\n",
    "D3 = \"mining massive dataset\"\n",
    "D4 = \"modern information retrieval\"\n",
    "D5 = \"search engine information retrieval in practice\"\n",
    "D6 = \"information retrieval implementing and evaluating search engine\"\n",
    "\n",
    "Collection = {\"DOC1\": D1,\n",
    "            \"DOC2\": D2, \n",
    "            \"DOC3\": D3, \n",
    "            \"DOC4\": D4 ,\n",
    "            \"DOC5\": D5,\n",
    "             \"DOC6\" : D6}\n",
    "\n",
    "StopWords = [\"and\", \"to\",\"in\"]\n",
    "\n",
    "BooleanOperator = {'AND', 'OR', 'NOT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeiXwbpbj_uc",
    "outputId": "713e4a6b-bd6e-436e-fc71-9819dec23d46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOC1': 'information retrieval and massive data processing',\n",
       " 'DOC2': 'introduction to information retrieval',\n",
       " 'DOC3': 'mining massive dataset',\n",
       " 'DOC4': 'modern information retrieval',\n",
       " 'DOC5': 'search engine information retrieval in practice',\n",
       " 'DOC6': 'information retrieval implementing and evaluating search engine'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_vjolhnj_uk"
   },
   "source": [
    "1- Ecrire une fonction permettant de filtrer la collection des stop words et qui renvoie la collection filtrée.\n",
    "**Important : dans la correction donnée, la collection est aussi segmentée en tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1558305069419,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "dbOHCXYvNzOg",
    "outputId": "cd61fce5-ade0-477a-ace0-0daa7729864f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOC1': ['information', 'retrieval', 'massive', 'data', 'processing'], 'DOC2': ['introduction', 'information', 'retrieval'], 'DOC3': ['mining', 'massive', 'dataset'], 'DOC4': ['modern', 'information', 'retrieval'], 'DOC5': ['search', 'engine', 'information', 'retrieval', 'practice'], 'DOC6': ['information', 'retrieval', 'implementing', 'evaluating', 'search', 'engine']}\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_word(collection, stop_word_list):\n",
    "    filtered_collection ={}\n",
    "    for doc in collection:\n",
    "        filtered_collection[doc]=[]\n",
    "        for term in collection[doc].split():\n",
    "            if term not in stop_word_list:\n",
    "                filtered_collection[doc].append(term)\n",
    "    return filtered_collection\n",
    "\n",
    "\n",
    "# Application de la fonction sur la collection\n",
    "\n",
    "Filtered_Collection = remove_stop_word(Collection,StopWords)\n",
    "print(Filtered_Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGRSU9OvNzOo"
   },
   "source": [
    "2- Ecrire une fonction permettant d'extraire le vocabulaire extrait de la collection (termes uniques).\n",
    "\n",
    "**Correction** : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1558305073689,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "I35dYLKJNzOq",
    "outputId": "09f04b8d-b19c-46a0-eafa-836f1446c84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'dataset', 'engine', 'evaluating', 'implementing', 'information', 'introduction', 'massive', 'mining', 'modern', 'practice', 'processing', 'retrieval', 'search']\n"
     ]
    }
   ],
   "source": [
    "def extract_vocabulary(collection):\n",
    "    vocabulary = set()\n",
    "    for doc in collection:\n",
    "        for term in collection[doc]:\n",
    "            vocabulary.update([term])\n",
    "    return vocabulary\n",
    "\n",
    "# On récupère le document sous forme de liste triée par ordre lexicographique.\n",
    "Vocabulary =sorted(list(extract_vocabulary(Filtered_Collection)))\n",
    "print(Vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfAMOFTHNzOz"
   },
   "source": [
    " 3- Ecrire une fonction `term_document_incidence_matrix (Collection,Vocabulary)` qui construit et renvoie la matrice terme-incidence d'une collection donnée en paramètre.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhUN078ONzO2"
   },
   "outputs": [],
   "source": [
    "def term_document_incidence_matrix (Collection,Vocabulary):\n",
    "    term_document_matrix = {}\n",
    "    for term in Vocabulary:\n",
    "        term_document_matrix[term] = []\n",
    "        for doc in Collection:\n",
    "            if term in Collection[doc]:\n",
    "                 term_document_matrix[term].append(1)\n",
    "            else:\n",
    "                term_document_matrix[term].append(0)\n",
    "    return term_document_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQFtoghUNzO6"
   },
   "source": [
    "4- Appliquer cette fonction sur la collection filtrée des 6 documents et créer la matrice d'incidence en la nommant `matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1558305085140,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "CM2Fq61DNzO8",
    "outputId": "11910d53-4b7d-40b7-a4b0-b187d448ab3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [1, 0, 0, 0, 0, 0], 'dataset': [0, 0, 1, 0, 0, 0], 'engine': [0, 0, 0, 0, 1, 1], 'evaluating': [0, 0, 0, 0, 0, 1], 'implementing': [0, 0, 0, 0, 0, 1], 'information': [1, 1, 0, 1, 1, 1], 'introduction': [0, 1, 0, 0, 0, 0], 'massive': [1, 0, 1, 0, 0, 0], 'mining': [0, 0, 1, 0, 0, 0], 'modern': [0, 0, 0, 1, 0, 0], 'practice': [0, 0, 0, 0, 1, 0], 'processing': [1, 0, 0, 0, 0, 0], 'retrieval': [1, 1, 0, 1, 1, 1], 'search': [0, 0, 0, 0, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "matrix = term_document_incidence_matrix(Filtered_Collection,Vocabulary)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXrqd-zoNzPK"
   },
   "source": [
    "5- Ecrire une fonction `term_incidence_vector (term, matrix)` retournant le vecteur d'incidence d'un terme donné en paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1558305088219,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "yXcIdPrtNzPK",
    "outputId": "7c604dd8-f0a5-4d5f-9112-82fba00beac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 1]\n",
      "[1, 1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def term_incidence_vector(term,matrix):\n",
    "    return matrix[term]\n",
    "\n",
    "print(term_incidence_vector(\"information\",matrix))\n",
    "print(term_incidence_vector(\"retrieval\",matrix))\n",
    "print(term_incidence_vector(\"massive\",matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYpxfKknNzPS"
   },
   "source": [
    "La fonction donnée ci-dessous permet d'afficher un dictionnaire ligne par ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyft_TxONzPT"
   },
   "outputs": [],
   "source": [
    "def displayDict(D):\n",
    "    print(\"\\n\")\n",
    "    for i in D:\n",
    "        print (i , \" : \" ,D[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qowhjspiNzPY"
   },
   "source": [
    "Appliquer le code ci-dessous pour vérifier que la matrice d'incidence construite correspond bien à celle que vous avez construit manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1558305095731,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "PNOGJXrkNzPZ",
    "outputId": "6858a278-d377-4de6-8bed-01e36a921ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-Document incidence Matrix\n",
      "\n",
      "\n",
      "\n",
      "data  :  [1, 0, 0, 0, 0, 0]\n",
      "dataset  :  [0, 0, 1, 0, 0, 0]\n",
      "engine  :  [0, 0, 0, 0, 1, 1]\n",
      "evaluating  :  [0, 0, 0, 0, 0, 1]\n",
      "implementing  :  [0, 0, 0, 0, 0, 1]\n",
      "information  :  [1, 1, 0, 1, 1, 1]\n",
      "introduction  :  [0, 1, 0, 0, 0, 0]\n",
      "massive  :  [1, 0, 1, 0, 0, 0]\n",
      "mining  :  [0, 0, 1, 0, 0, 0]\n",
      "modern  :  [0, 0, 0, 1, 0, 0]\n",
      "practice  :  [0, 0, 0, 0, 1, 0]\n",
      "processing  :  [1, 0, 0, 0, 0, 0]\n",
      "retrieval  :  [1, 1, 0, 1, 1, 1]\n",
      "search  :  [0, 0, 0, 0, 1, 1]\n",
      "\n",
      "\n",
      "Incidence Vector of information [1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Affichage de la matrice terme incidence \n",
    "print(\"Term-Document incidence Matrix\\n\")\n",
    "displayDict(term_document_incidence_matrix (Filtered_Collection,Vocabulary))\n",
    "print (\"Incidence Vector of information\", term_incidence_vector('information',matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNCaKrzyNzPh"
   },
   "source": [
    "6- Ecrire une fonction `query_pre_processing(query)` qui prend en entrée une requête sous la forme d'une chaîne de caractères et qui renvoie la représentation d'une requête sous la forme d'une liste de termes et d'opérateurs booléens définis ici par : `BooleanOperator = {'AND', 'OR', 'NOT'}`.\n",
    "Par exemple pour la requête `\"information AND retrieval AND NOT massive\"` la fonction renverra la liste `[ \"information\", \"AND\", \"retrieval\", \"AND\", \"NOT\", \"massive\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1558305099270,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "xn_mYosjNzPi",
    "outputId": "db1aa2bd-5a35-45e2-cb41-c4a9bf00946b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['information', 'AND', 'retrieval', 'AND', 'NOT', 'massive']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_pre_processing(query):\n",
    "    processed_query=[]\n",
    "    for item in query.split():\n",
    "        processed_query.append(item)\n",
    "    return processed_query\n",
    "\n",
    "\n",
    "query_pre_processing(\"information AND retrieval AND NOT massive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKPpy0IlNzPo"
   },
   "source": [
    "On considère la fonction `boolean_operator_processing (BoolOperator,term1,term2)` permettant de calculer le résultat de `term1 BoolOperator term2`. Cette fonction utilise la fonction `zip` de python décrite [ici](https://docs.python.org/3.3/library/functions.html#zip).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G34IHBp_NzPq"
   },
   "outputs": [],
   "source": [
    "def boolean_operator_processing(BoolOperator,term1,term2):\n",
    "    result=[]\n",
    "    if BoolOperator == \"AND\":\n",
    "        for a , b in zip(term1,term2) :\n",
    "            if a==1 and b==1 :\n",
    "                result.append(1)\n",
    "            else :\n",
    "                result.append(0)\n",
    "    elif BoolOperator==\"OR\" :\n",
    "        for a,b in zip(term1,term2)  :\n",
    "            if a==0 and b==0 :\n",
    "                result.append(0)\n",
    "            else :\n",
    "                result.append(1)\n",
    "    elif BoolOperator == \"NOT\":\n",
    "        for b in term1 :\n",
    "            if b == 1 :\n",
    "                result.append(0)\n",
    "            else :\n",
    "                result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkJsxJRNNzPt"
   },
   "source": [
    "Appliquer le code ci-dessous pour tester cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1558305107573,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "FM-DdULgNzPx",
    "outputId": "de02d049-850d-4b78-db8c-77c6f37cb1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "v1=[0,1,0,1]\n",
    "v2=[0,0,1,1]\n",
    "v3=[] #Astuce pour l'opération NOT\n",
    "print(boolean_operator_processing(\"AND\",v1,v2))\n",
    "print(boolean_operator_processing(\"OR\",v1,v2))\n",
    "print(boolean_operator_processing(\"NOT\",v1,v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXfbxCFjNzP7"
   },
   "source": [
    "7- Ecrire une fonction `query_processing (term_incidence_matrix, query, booleanOperator)` permettant de traiter la requête pré-processée `query` en utilisant la matrice `term_incidence_matrix` et donc en appliquant les opérations binaires sur les vecteurs d'incidence des différents termes de la requête. \n",
    "\n",
    "**Astuce : on supposera que la requête est donnée sous sa [forme normale conjonctive](https://fr.wikipedia.org/wiki/Forme_normale_conjonctive) en notation polonaise inversée pour faciliter son evaluation (c.f. Partie 2 du LAB).** Par exemple, la forme normale conjonctive en notation polonaise inversée de la requête \"information AND retrieval AND NOT massive\" devient \"information retrieval AND massive NOT AND\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-YSaT3dNzP8"
   },
   "outputs": [],
   "source": [
    "def query_processing (term_incidence_matrix, query, booleanOperator):\n",
    "    evaluation_stack = []\n",
    "    for term in query:\n",
    "        if term not in booleanOperator:\n",
    "            evaluation_stack.append(term_incidence_matrix[term])\n",
    "        else:\n",
    "            if term == \"NOT\":\n",
    "                eval_prop = boolean_operator_processing(term, evaluation_stack.pop(),[])\n",
    "            else:\n",
    "                eval_prop = boolean_operator_processing(term, evaluation_stack.pop(),evaluation_stack.pop())\n",
    "            evaluation_stack.append(eval_prop)\n",
    "    return  evaluation_stack.pop()\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypRPEUgVNzQD"
   },
   "source": [
    "8- Appliquer cette fonction aux 3 requêtes de l'exercice 2 et vérifier que le résultat est le même que celui calculé à la main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1558305115566,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "Ij05UI50NzQG",
    "outputId": "bd276d9b-30ea-40de-9faa-003027bc23ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reponse à la requete 1 : [0, 1, 0, 1, 1, 1]\n",
      "Reponse à la requete 2 : [0, 0, 0, 0, 0, 1]\n",
      "Reponse à la requete 3 : [1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reponse à la requete 1 : {}\".format(query_processing(matrix,query_pre_processing(\"information retrieval AND massive NOT AND\"),BooleanOperator)))\n",
    "print(\"Reponse à la requete 2 : {}\".format(query_processing(matrix,query_pre_processing(\"search engine AND practice NOT AND\"),BooleanOperator)))\n",
    "print(\"Reponse à la requete 3 : {}\".format(query_processing(matrix,query_pre_processing(\"information search OR retrieval AND\"),BooleanOperator)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ieTRPxrj_um"
   },
   "source": [
    "### MODELE VECTORIEL\n",
    "**Exercice 4**\n",
    "\n",
    "**LA CORRECTION complète de l'exercice est donnée dans un document pdf déposé sur EDUNAO. Quelques élements de correction sous forme de code python sont donnés ci-dessous.**\n",
    "\n",
    "Considérons une requête $q$ contenant les terme *os, Jaguar* et trois documents de même taille $d_1$, $d_2$ et $d_3$ qui contiennent respectivement :\n",
    " + *Jaguar, Jaguar, Jungle, Jungle, Jungle*\n",
    " + *Système d'exploitation, Jaguar, Mac, Système d'exploitation, Système d'exploitation*\n",
    " + *Jaguar, Bentley, Mercedes, Jaguar, Jaguar* \n",
    " \n",
    "plus précisément :\n",
    "\n",
    "`q = {os, Jaguar}`\n",
    "\n",
    "`d_1 = {Jaguar, Jaguar, Jungle, Jungle, Jungle}`\n",
    "\n",
    "`d_2 = {Système d'exploitation, Jaguar, Mac, Système d'exploitation, Système d'exploitation}`\n",
    "\n",
    "`d_3 = {Jaguar, Bentley, Mercedes, Jaguar, Jaguar}`\n",
    "\n",
    "Dans la suite, nous utiliserons l'abréviation S.E. pour *Système d'exploitation* et nous supposerons que le vocabulaire associé est : \n",
    "\n",
    "$$\\mathcal{V} = \\{ bentley, jaguar, jungle, mac, mercedes, os, S.E.\\} $$\n",
    "\n",
    "**1. Donner les vecteurs associés aux documents et à la requête.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1558305119863,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "FOORmFNRNzQO",
    "outputId": "61c33fa7-423f-47f4-86bd-569b1002760e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affichage de la collection\n",
      "\n",
      "\n",
      "d_1  :  ['jaguar', 'jaguar', 'jungle', 'jungle', 'jungle']\n",
      "d_2  :  ['S.E.', 'jaguar', 'mac', 'S.E.', 'S.E.']\n",
      "d_3  :  ['jaguar', 'bentley', 'mercedes', 'jaguar', 'jaguar']\n",
      "\n",
      "\n",
      "Representation vectorielle de d_1 : [0, 2, 3, 0, 0, 0, 0]\n",
      "Representation vectorielle de d_2 : [0, 1, 0, 1, 0, 0, 3]\n",
      "Representation vectorielle de d_3 : [1, 3, 0, 0, 1, 0, 0]\n",
      "Representation vectorielle de la requete q : os,jaguar\n"
     ]
    }
   ],
   "source": [
    "collection_exo = {'d_1': ['jaguar', 'jaguar', 'jungle', 'jungle', 'jungle'], \n",
    "                   'd_2': ['S.E.','jaguar','mac','S.E.','S.E.'],\n",
    "                   'd_3': ['jaguar','bentley','mercedes','jaguar','jaguar']}\n",
    "\n",
    "print(\"Affichage de la collection\")\n",
    "displayDict(collection_exo)\n",
    "\n",
    "vocabulary_exo=['bentley', 'jaguar', 'jungle', 'mac', 'mercedes', 'os', 'S.E.']\n",
    "\n",
    "\n",
    "def vectorial_representation_doc(collection,doc,vocabulary):\n",
    "    vector_doc = []\n",
    "    for term in vocabulary:\n",
    "        count_term=0\n",
    "        for token in collection[doc]:\n",
    "            if token==term:\n",
    "                count_term=count_term+1\n",
    "        vector_doc.append(count_term)\n",
    "    return vector_doc\n",
    "\n",
    "def vectorial_representation_query(query,vocabulary):\n",
    "    query_vector=[]\n",
    "    for term in vocabulary:\n",
    "        count_term=0\n",
    "        for token in query.split(\",\"):\n",
    "            if token==term:\n",
    "                count_term=count_term+1\n",
    "        query_vector.append(count_term)\n",
    "    return query_vector\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Representation vectorielle de {} : {}\".format('d_1',vectorial_representation_doc(collection_exo,'d_1',vocabulary_exo)))\n",
    "print(\"Representation vectorielle de {} : {}\".format('d_2',vectorial_representation_doc(collection_exo,'d_2',vocabulary_exo)))\n",
    "print(\"Representation vectorielle de {} : {}\".format('d_3',vectorial_representation_doc(collection_exo,'d_3',vocabulary_exo)))\n",
    "\n",
    "print(\"Representation vectorielle de la requete {} : {}\".format('q', 'os,jaguar',vectorial_representation_query('os,jaguar',vocabulary_exo)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdtg2sCoNzQX"
   },
   "source": [
    "**2. Dans le cas où on privilégie une représentation à base de $tf$, ordonner les documents par rapport à leur score (i.e. produit scalaire) avec la requête.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1558305127121,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "r28paQTdNzQb",
    "outputId": "2dc05559-49bb-4f90-e03d-ff9b4782d1e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d_3', 3), ('d_1', 2), ('d_2', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_score(dico_item):\n",
    "    return dico_item[1]\n",
    "\n",
    "\n",
    "def rank_document (collection,query,vocabulary):\n",
    "    scores = {}\n",
    "    for doc in collection:\n",
    "        doc_vec = np.array(vectorial_representation_doc(collection,doc,vocabulary))\n",
    "        query_vec = np.array(vectorial_representation_query(query,vocabulary))\n",
    "        scores[doc]=np.vdot(doc_vec,query_vec)\n",
    "    ranks = sorted(scores.items(), key=get_score, reverse=True)\n",
    "    return ranks\n",
    "    \n",
    "rank_document(collection_exo,'os,jaguar',vocabulary_exo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6gEt8gLNzQk"
   },
   "source": [
    "*Jaguar* est un terme polysémique et on voit bien sur l'exemple précédent que si un terme polysémique d'une requête est répété plusieurs fois dans des documents traitant d'autres sujets que ce que l'on recherche, ces documents obtiendront un meilleur score que ceux traitant du sujet mais contenant moins d'occurrences du terme polysémique. Une solution est d'augmenter la couverture des termes du vocabulaire en prenant en compte, dans la représentation vectorielle des documents de la requête, les termes synonymes de ceux apparaissant dans les documents et la requête. Un moyen simple pour cela consiste à définir une matrice de similarité $W$ entre les termes et de projeter les documents et la requête sur cette matrice avant de calculer leurs scores. Pour notre exemple, considérons la matrice de similarité entre les termes suivante :\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "0.5 & 0.1 & 0 & 0 & 0.4 & 0 & 0 \\\\\n",
    "0.1 & 0.5 & 0.05 & 0.05 & 0.1 & 0.1 & 0.1 \\\\\n",
    "0 & 0.05 & 0.95 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0.05 & 0 & 0.8 & 0 & 0.05 &  0.1 \\\\\n",
    "0.4 & 0.1 & 0 & 0 & 0.5 & 0 & 0 \\\\\n",
    "0 & 0.1 & 0 & 0.05 & 0 & 0.55 & 0.3 \\\\\n",
    "0 & 0.1 & 0 & 0.1 & 0 & 0.3 & 0.5\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "**3. Quelles sont les nouvelles représentations des documents $d_1$, $d_2$ et $d_3$ ainsi que de la requête $q$ ? Calculer les nouveaux scores produits scalaires entre ces documents et $q$ et ordonner ces derniers par rapport à ces scores. Conclure.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1558305132651,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "zz8CjQ9WNzQl",
    "outputId": "cf85f0b7-e07f-4eb8-e16e-9ddb6720a0ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5  0.1  0.   0.   0.4  0.   0.  ]\n",
      " [0.1  0.5  0.05 0.05 0.1  0.1  0.1 ]\n",
      " [0.   0.05 0.95 0.   0.   0.   0.  ]\n",
      " [0.   0.05 0.   0.8  0.   0.05 0.1 ]\n",
      " [0.4  0.1  0.   0.   0.5  0.   0.  ]\n",
      " [0.   0.1  0.   0.05 0.   0.55 0.3 ]\n",
      " [0.   0.1  0.   0.1  0.   0.3  0.5 ]]\n",
      "Nouvelles representations\n",
      "q : [0.1  0.6  0.05 0.1  0.1  0.65 0.4 ]\n",
      "d_1 : [0.2  1.15 2.95 0.1  0.2  0.2  0.2 ]\n",
      "d_2 : [0.1  0.85 0.05 1.15 0.1  1.05 1.7 ]\n",
      "d_3 : [1.2  1.7  0.15 0.15 1.2  0.3  0.3 ]\n",
      "Nouvel ordonnancement\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('d_2', 2.0100000000000002), ('d_3', 1.5975000000000001), ('d_1', 1.0975)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W =np.array([[0.5,0.1,0, 0, 0.4,0,0],[0.1,0.5, 0.05, 0.05, 0.1,0.1,0.1],[0,0.05, 0.95, 0, 0,0,0],[0,0.05, 0, 0.8, 0,0.05,0.1],[0.4,0.1, 0, 0, 0.5,0,0],[0,0.1, 0, 0.05, 0,0.55,0.3],[0,0.1, 0, 0.1, 0,0.3,0.5]])\n",
    "print(W)\n",
    "\n",
    "print(\"Nouvelles representations\")\n",
    "\n",
    "new_query_vec =np.dot(W,vectorial_representation_query('os,jaguar',vocabulary_exo))\n",
    "print(\"q : {}\".format(new_query_vec ))\n",
    "new_doc_1_vec = np.dot(W,vectorial_representation_doc(collection_exo,'d_1',vocabulary_exo))\n",
    "print (\"{} : {}\".format('d_1',new_doc_1_vec))\n",
    "new_doc_2_vec = np.dot(W,vectorial_representation_doc(collection_exo,'d_2',vocabulary_exo))\n",
    "print (\"{} : {}\".format('d_2',new_doc_2_vec))\n",
    "new_doc_3_vec = np.dot(W,vectorial_representation_doc(collection_exo,'d_3',vocabulary_exo))\n",
    "print (\"{} : {}\".format('d_3',new_doc_3_vec))\n",
    "\n",
    "\n",
    "print(\"Nouvel ordonnancement\")\n",
    "\n",
    "\n",
    "def rank_document (collection,query,vocabulary, similarity_matrix):\n",
    "    scores = {}\n",
    "    for doc in collection:\n",
    "        doc_vec = np.dot(similarity_matrix,np.array(vectorial_representation_doc(collection,doc,vocabulary)))\n",
    "        query_vec = np.dot(similarity_matrix,np.array(vectorial_representation_query(query,vocabulary)))\n",
    "        scores[doc]=np.vdot(doc_vec,query_vec)\n",
    "    ranks = sorted(scores.items(), key=get_score, reverse=True)\n",
    "    return ranks\n",
    "    \n",
    "rank_document(collection_exo,'os,jaguar',vocabulary_exo,W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate bien qu’avec l’information de similarité, les termes du vocabulaire similaires à ceux de la requête mais absents de cette dernière auront des poids non nuls dans le vecteur de représentation de la requête (exemple des termes os et S.E.).\n",
    "Avec cette nouvelle représentation, les documents qui ne contiennent pas exactement les termes de la requête, mais qui sont pertinents auront ainsi des similarités non nulles avec cette dernière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnG33sWJNzQr"
   },
   "source": [
    "**4. Si on suppose que les termes qui apparaissent dans les mêmes documents avec les mêmes fréquences sont sémantiquement similaires, donner un moyen simple de calculer la matrice de similarité entre termes notée $W$.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dD282ePMNzQs"
   },
   "source": [
    "Avec $D.D^T$ où D est la matrice termes-documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wuYzeBDj_up"
   },
   "source": [
    "\n",
    "## Partie 2  : Mise en oeuvre sur la collection TIME\n",
    "\n",
    "\n",
    "Dans cette partie, il s'agit de mettre en oeuvre les différents modèles de recherche sur la collection TIME en utilisant la représentation de la collection sous la forme d'un index inversé et les différents algorithmes vus en cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAwS7TJ9NzQ3"
   },
   "source": [
    "1- Charger l'index inversé de la collection TIME construit et sauvegardé sous la forme d'un fichier dans le LAB1. Les différents index sont disponibles [ici](./Index). Nous avons aussi mis dans le répertoire [Utils](./Utils) un ensemble de modules python permettant de charger les index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 443,
     "status": "error",
     "timestamp": 1558305153208,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "8CBtPjMwNzQ3",
    "outputId": "5839ed4e-cc28-45a7-f7a8-ff73b724739a"
   },
   "outputs": [],
   "source": [
    "from Utils.saveandload_pickle import *\n",
    "\n",
    "document_index = load_inverted_index_pickle('./Index/index_document.pickle')\n",
    "#print(document_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5PsGu_cNzQ9"
   },
   "source": [
    "#### Modèle booléen\n",
    "\n",
    "**Représentation des requêtes**\n",
    "\n",
    "Une des premières étapes pour la mise en oeuvre du modèle booléen est la représentation d'une requête. En effet, dans le modèle booléen, les requêtes sont des expressions booléennes pouvant être définies à l'aide des opérateurs logiques `OR`, `NOT` et `AND` comme par exemple la requête ` U.S AND War OR France`.  Nous avons vu dans le cours qu'il pouvait être intéressant de représenter les requêtes par leur [forme normale conjonctive](https://fr.wikipedia.org/wiki/Forme_normale_conjonctive) (conjonction de disjonctions) afin d'optimiser leur traitement à l'aide de l'index inversé. Dans la suite, nous supposerons donc que les requêtes sont exprimées souc cette forme. \n",
    "\n",
    "2- **Quelle structure de données informatique proposez-vous pour representer une requête sous forme normale conjonctive et faciliter son traitement ?** Puis, à partir de cette représentation, quelle stratégie mettre en place pour évaluer l'expression booléenne associée ?\n",
    "\n",
    "Indice :\n",
    "\n",
    "\n",
    "<img src=\"./Figures/booleantree.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "Une manière de faire pourrait être de récupérer les littéraux `U.S`et `France `et de leur appliquer l'opérateur `OR` puis de récupérer le littéral `War`de lui appliquer `NOT` et enfin d'appliquer l'opérateur `AND` aux deux résultats des évaluations précédentes. Si on écrit la formule selon cette stratégie, on obtient :\n",
    "` U.S France OR War NOT AND`. \n",
    "Cela correspond à la notation post-fixe de la formule. Cette notation, aussi connue sous le nom de [notation polonaise inversée](https://fr.wikipedia.org/wiki/Notation_polonaise_inverse) (en hommage à son créateur Jan Łukasiewicz grand logicien et philosophe polonais) est très pratique pour l'évaluation de la formule. Elle permet aussi de ne pas utiliser de parenthèses sans ambiguité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPDutwRGNzQ-"
   },
   "source": [
    "Transformer une expression en sa représentation sous forme d'arbre ou en sa notation post-fixe est un beau problème d'algorithmique et de programmation et vous pourrez y reflechir à la maison. Dans le Lab, nous éviter de passer trop de temps sur cette question, nous utiliserons la bibliothèque [`tt`](http://tt.brianwel.ch/en/latest/index.html) qui permet de travailler avec des expressions en logique booléenne en python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VPY4ZLtNzQ-"
   },
   "source": [
    "Installer la bibliothéque en executant la commande ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2KCzdQ-5NzRA",
    "outputId": "c201e667-df77-40b3-bace-e2e38b6fe3b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/myriamtami/Library/Caches/pip/wheels/d1/f0/46/e03ef87636ec3f61f5933eba6deb9fb8acd1bf436c600d6dc2/ttable-0.6.3-cp37-cp37m-macosx_10_7_x86_64.whl\n",
      "Installing collected packages: ttable\n",
      "Successfully installed ttable-0.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ttable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ejbq8vjNzRJ"
   },
   "source": [
    "Le code ci-dessous vous permet de transformer une requête exprimée sous la forme d'une chaîne de caractères en une formule booléeen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqPGd9hKNzRK",
    "outputId": "4b9e1927-f85d-4d44-a29c-0fc89feb62a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(War or France) and (not C)\n"
     ]
    }
   ],
   "source": [
    "from tt import BooleanExpression\n",
    "\n",
    "b = BooleanExpression('(War or France) and (not C)')\n",
    "print(b)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZ2lArQMNzRY"
   },
   "source": [
    "L'instruction ci-dessous permet de récupérer l'arbre représentant la formule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWT3qDKfNzRZ",
    "outputId": "1e8fe3fb-352f-42b3-8e17-cbed0a0a4c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "`----or\n",
      "|    `----War\n",
      "|    `----France\n",
      "`----not\n",
      "     `----C\n"
     ]
    }
   ],
   "source": [
    "b.tree\n",
    "print(b.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21qOyyiHNzRr"
   },
   "source": [
    "Il est aussi possible de récupérer l'ensemble des tokens avec l'instruction ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9H1C_GtjNzRs",
    "outputId": "caedfd29-fe2b-4618-e957-23a849ab5661"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', 'War', 'or', 'France', ')', 'and', '(', 'not', 'C', ')']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " b.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ky-5_RdDNzRv"
   },
   "source": [
    "De même, il est possible de récupérer l'ensemble des tokens dans l'ordre post-fixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95TScfneNzRw",
    "outputId": "11ac2072-342f-40f3-bf7e-1c8328ef60ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['War', 'France', 'or', 'C', 'not', 'and']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.postfix_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XffwW52CNzR1"
   },
   "source": [
    "Dans la suite, on considérera donc deux cas:\n",
    "\n",
    "+ 1er cas : les requêtes sont données en langage naturel (comme par exemple dans le fichier [TIME.QUE](./Data/Time/TIME.QUE)), fournies avec la collection TIME et il faudra donc transformer cette requête en expression logique. En particulier, on considèrera que l'espace correspond à l'opérateur `AND`. Par exemple, la requête `KENNEDY ADMINISTRATION PRESSURE` correspond à la requête `kennedy AND administration AND pressure`. \n",
    "\n",
    "3- Ecrire une fonction `def transformation_query_to_boolean(query)` qui permet de transformer une requête en langage naturel sous sa forme logique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I970DuQCNzR3",
    "outputId": "747d26e1-bc37-483e-d97e-1acfa5c3ee98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KENNEDY', 'AND', 'ADMINISTRATION', 'AND', 'PRESSURE']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformation_query_to_boolean(query):\n",
    "    boolean_query=[]\n",
    "    for token in query.split():\n",
    "        boolean_query.append(token)\n",
    "        boolean_query.append('AND')\n",
    "    boolean_query.pop()\n",
    "    return boolean_query\n",
    "\n",
    "\n",
    "transformation_query_to_boolean(\"KENNEDY ADMINISTRATION PRESSURE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTfJf1skNzR6"
   },
   "source": [
    " + 2ème cas : les requêtes sont fournies comme une expression logique (par exemple `'(War or France) and (not C)'`) et il s'agit de la transformer sous une forme post-fixe pour son evaluation.\n",
    "\n",
    "4- En utilisant la bibliothèque `tt` comme montré précedemment, écrire une fonction permettant de transformer une requête en son ensemble de tokens ordonnés par ordre post-fixe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5C46c-8SNzR6",
    "outputId": "e6b46ab6-c106-4176-9916-13db7c5aca8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['War', 'France', 'or', 'C', 'not', 'and']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt import BooleanExpression\n",
    "\n",
    "def transformation_query_to_postfixe(query):\n",
    "    b = BooleanExpression(query)\n",
    "    return b.postfix_tokens\n",
    "\n",
    "transformation_query_to_postfixe('(War or France) and (not C)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxsJxRGzNzSC"
   },
   "source": [
    "**Traitement des requêtes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fm5DThZNzSC"
   },
   "source": [
    "Il s'agit ici d'écrire une fonction `processing_boolean_query_with_inverted_index(booleanOperator,query, inverted_index)`\n",
    "qui permet de renvoyer la liste de documents pertinents pour la requête `query`. `booleanOperator`est l'ensemble des opérateurs booléens considérés et `inverted_index` est l'index inversé de la collection considérée.\n",
    "\n",
    "Quelques indications pour vous aider :\n",
    " + L'utilisation d'une structure de données [Pile](https://docs.python.org/3/tutorial/datastructures.html#using-lists-as-stacks) et de la notation polonaise inversée peut être utile pour l'évaluation de la requête complète.\n",
    " + Revoir le cours 1 sur le modèle booléen pour l'utilisation de l'index inversé dans ce cas.\n",
    " \n",
    "**On suppose que les listes de postings de l'index inversé sont triées par doc_id croissant**\n",
    "\n",
    "5- Pour faciliter l'écriture de cette fonction, nous allons d'abord écrire les fonctions :\n",
    "+ `def merge_and_postings_list(posting_term1,posting_term2)` qui applique l'opérateur AND à deux postings listes : intersection de posting liste (c.f. cours) ;\n",
    "+ `def merge_or_postings_list(posting_term1,posting_term2)` qui applique l'opérateur OR à deux postings listes : union de posting liste (c.f. cours) ;\n",
    "+ `def merge_and_not_postings_list(posting_term1,posting_term2)`qui calcule posting_term1 AND NOT posting_term2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9X_WHdTNzSC",
    "outputId": "722ed9f1-9ad1-4910-a9e9-e2e1d3e1d314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postings du terme NASSAU : ['017', '040', '045', '071', '183', '196', '308']\n",
      "Postings du terme FRANCE : ['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '310', '317', '323', '326', '336', '353', '357', '367', '380', '392', '402', '404', '408', '412', '434', '436', '463', '470', '471', '477', '504', '511', '513', '519', '528', '530', '537', '541', '550']\n",
      "Test du AND\n",
      "['017', '040', '045', '071', '183']\n",
      "############################################\n",
      "Postings du terme NASSAU : ['017', '040', '045', '071', '183', '196', '308']\n",
      "Postings du terme FRANCE : ['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '310', '317', '323', '326', '336', '353', '357', '367', '380', '392', '402', '404', '408', '412', '434', '436', '463', '470', '471', '477', '504', '511', '513', '519', '528', '530', '537', '541', '550']\n",
      "Test du OR\n",
      "['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '196', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '308']\n",
      "############################################\n",
      "Postings du terme NASSAU : ['017', '040', '045', '071', '183', '196', '308']\n",
      "Postings du terme FRANCE : ['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '310', '317', '323', '326', '336', '353', '357', '367', '380', '392', '402', '404', '408', '412', '434', '436', '463', '470', '471', '477', '504', '511', '513', '519', '528', '530', '537', '541', '550']\n",
      "Test du NASSAU AND_NOT FRANCE\n",
      "['196', '308']\n",
      "############################################\n",
      "Postings du terme NASSAU : ['017', '040', '045', '071', '183', '196', '308']\n",
      "Postings du terme FRANCE : ['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '310', '317', '323', '326', '336', '353', '357', '367', '380', '392', '402', '404', '408', '412', '434', '436', '463', '470', '471', '477', '504', '511', '513', '519', '528', '530', '537', '541', '550']\n",
      "Test du FRANCE AND_NOT NASSAU\n",
      "['026', '030', '049', '050', '055', '061', '062', '067', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '188', '194', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299']\n"
     ]
    }
   ],
   "source": [
    "def merge_and_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "# Test \n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du AND\")\n",
    "res = merge_and_postings_list(document_index[\"NASSAU\"], document_index[\"FRANCE\"])\n",
    "print(res)\n",
    "print(\"############################################\")\n",
    "\n",
    "def merge_or_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                result.append(posting_term2[j])\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test \n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du OR\")\n",
    "res = merge_or_postings_list(document_index[\"NASSAU\"], document_index[\"FRANCE\"])\n",
    "print(res)\n",
    "print(\"############################################\")\n",
    "\n",
    "def merge_and_not_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test \n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du NASSAU AND_NOT FRANCE\")\n",
    "res = merge_and_not_postings_list(document_index[\"NASSAU\"], document_index[\"FRANCE\"])\n",
    "print(res)\n",
    "print(\"############################################\")\n",
    "\n",
    "\n",
    "# ATTENTION à l'ordre des opérandes\n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "print(\"Test du FRANCE AND_NOT NASSAU\")\n",
    "res = merge_and_not_postings_list(document_index[\"FRANCE\"], document_index[\"NASSAU\"])\n",
    "print(res)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdoIRgHeNzSI"
   },
   "source": [
    "6- En utilisant les fonctions implémentées dans la question précédente, écrire la fonction `boolean_operator_processing_with_inverted_index(BoolOperator,posting_term1,posting_term2)` qui applique l'opérateur `BoolOperator` parmi (AND, OR et AND NOT) à deux posting listes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBltE0nkNzSJ",
    "outputId": "ec9abc86-2622-49e6-eba3-1f2551a6ec3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postings du terme NASSAU : ['017', '040', '045', '071', '183', '196', '308']\n",
      "Postings du terme FRANCE : ['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '310', '317', '323', '326', '336', '353', '357', '367', '380', '392', '402', '404', '408', '412', '434', '436', '463', '470', '471', '477', '504', '511', '513', '519', '528', '530', '537', '541', '550']\n",
      "test de la fonction globale\n",
      "[['017', '026', '030', '040', '045', '049', '050', '055', '061', '062', '067', '071', '072', '085', '086', '088', '098', '102', '113', '126', '128', '129', '131', '143', '146', '150', '158', '159', '163', '172', '180', '181', '183', '188', '194', '196', '197', '199', '200', '213', '218', '220', '225', '226', '231', '236', '251', '260', '261', '267', '274', '280', '282', '284', '287', '292', '293', '299', '308']]\n"
     ]
    }
   ],
   "source": [
    "def boolean_operator_processing_with_inverted_index(BoolOperator,posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    if BoolOperator == \"AND\":\n",
    "        result.append(merge_and_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator==\"OR\" :\n",
    "        result.append(merge_or_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator == \"NOT\":\n",
    "        result.append(merge_and_not_postings_list(posting_term1,posting_term2))\n",
    "    return result\n",
    "\n",
    "\n",
    "BooleanOperator = {\"AND\", \"OR\", \"NOT\"}\n",
    "print(\"Postings du terme {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"Postings du terme {} : {}\".format('FRANCE',document_index['FRANCE']))\n",
    "\n",
    "\n",
    "print(\"test de la fonction globale\")\n",
    "#test de la fonction précédente\n",
    "result = boolean_operator_processing_with_inverted_index('OR',document_index['NASSAU'],document_index['FRANCE'])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ixMMS_xNzSP"
   },
   "source": [
    "7- Ecrire la fonction `def processing_boolean_query_with_inverted_index(booleanOperator,query, inverted_index)` qui permet de renvoyer la liste de documents pertinents pour la requête `query`. `booleanOperator`est l'ensemble des opérateurs booléens considérés et `inverted_index` est l'index inversé de la collection considérée.\n",
    "\n",
    "\n",
    "Pour la fonction permettant de traiter une requête, on procédera comme mentionné à la question 2 avec l'aide de la notation polonaise inversée et avec une struture de données PILE. **Attention, ici AND NOT est considéré comme un seul opérateur logique et non pas 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccjzCySpNzSQ",
    "outputId": "e6b7a941-f0b2-4567-e652-068d2cc7898b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test du modèle sur la requête : (Nassau or December) and (not Allies)\n",
      "requête : ['Nassau', 'December', 'or', 'Allies', 'not', 'and']\n",
      "posting list de NASSAU : ['017', '040', '045', '071', '183', '196', '308']\n",
      "posting list de DECEMBER : ['017', '070', '086', '138', '170', '183', '192', '204', '213', '272', '308', '337', '418', '426']\n",
      "posting list de ALLIES : ['017', '047', '058', '071', '085', '088', '100', '102', '107', '122', '126', '163', '181', '183', '213', '226', '230', '231', '252', '287', '317', '348', '353', '365', '392', '449', '495', '503', '504', '509', '511', '540', '552', '558']\n",
      "Liste des documents retournés : ['040', '045', '070', '086', '138', '170', '192', '196', '204', '272', '308']\n",
      "test du modèle sur la requête : Nassau or Nassau\n",
      "requête : ['Nassau', 'Nassau', 'or']\n",
      "Liste des documents retournés : ['017', '040', '045', '071', '183', '196', '308']\n",
      "test du modèle sur la requête : Nassau and (not Nassau)\n",
      "requête : ['Nassau', 'Nassau', 'not', 'and']\n",
      "Liste des documents retournés : []\n"
     ]
    }
   ],
   "source": [
    "def processing_boolean_query_with_inverted_index(booleanOperator,query, inverted_index):\n",
    "    evaluation_stack = []\n",
    "    for term in query:\n",
    "        if term.upper() not in booleanOperator:\n",
    "            evaluation_stack.append(inverted_index[term.upper()])\n",
    "        else:\n",
    "            if term.upper() == \"NOT\":\n",
    "                operande= evaluation_stack.pop()\n",
    "                eval_prop = boolean_operator_processing_with_inverted_index(term.upper(), evaluation_stack.pop(),operande)\n",
    "                evaluation_stack.append(eval_prop[0])\n",
    "                evaluation_stack.append(eval_prop[0])\n",
    "            else:\n",
    "                operator = term.upper()\n",
    "                eval_prop =  boolean_operator_processing_with_inverted_index(operator, evaluation_stack.pop(),evaluation_stack.pop())\n",
    "                evaluation_stack.append(eval_prop[0])\n",
    "    return  evaluation_stack.pop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "BooleanOperator = {'AND', 'NOT', 'OR'}\n",
    "\n",
    "\n",
    "# Test du modèle booléen\n",
    "\n",
    "print(\"test du modèle sur la requête : {}\".format('(Nassau or December) and (not Allies)'))\n",
    "\n",
    "query = transformation_query_to_postfixe('(Nassau or December) and (not Allies)')\n",
    "print(\"requête : {}\".format(query))\n",
    "print(\"posting list de {} : {}\".format('NASSAU',document_index['NASSAU']))\n",
    "print(\"posting list de {} : {}\".format('DECEMBER',document_index['DECEMBER']))\n",
    "print(\"posting list de {} : {}\".format('ALLIES',document_index['ALLIES']))\n",
    "print(\"Liste des documents retournés : {}\".format(processing_boolean_query_with_inverted_index(BooleanOperator,query,document_index)))\n",
    "\n",
    "\n",
    "print(\"test du modèle sur la requête : {}\".format('Nassau or Nassau'))\n",
    "\n",
    "query = transformation_query_to_postfixe('Nassau or Nassau')\n",
    "print(\"requête : {}\".format(query))\n",
    "print(\"Liste des documents retournés : {}\".format(processing_boolean_query_with_inverted_index(BooleanOperator,query,document_index)))\n",
    "\n",
    "\n",
    "print(\"test du modèle sur la requête : {}\".format('Nassau and (not Nassau)'))\n",
    "\n",
    "query = transformation_query_to_postfixe('Nassau and (not Nassau)')\n",
    "print(\"requête : {}\".format(query))\n",
    "print(\"Liste des documents retournés : {}\".format(processing_boolean_query_with_inverted_index(BooleanOperator,query,document_index)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1i6cTphLNzSS"
   },
   "source": [
    "#### Modèle vectoriel\n",
    "\n",
    "**Représentation des requêtes**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m85x_WXUNzSS"
   },
   "source": [
    "Dans le modèle vectoriel, les requêtes et les documents ont le même type de représentation, un vecteur pondéré dans l'espace des termes d'indexation. Il est donc nécessaire de faire subir à votre requête, souvent exprimée en langage naturel, les mêmes traitements que ceux appliqués aux documents lors de la phase d'indexation,  soit :\n",
    "+ L'étape de segmentation ;\n",
    "+ L'étape de filtrage ;\n",
    "+ L'étape de normalisation.\n",
    "\n",
    "Cela permettra notamment de garantir que l'espace de représentation de la requête et du document sont les mêmes.\n",
    "\n",
    "8- Ecrire une fonction `pre_processed_query(query)`qui prend en entrée une requête sous la forme d'une chaîne de caractères et qui renvoie la requête sous la forme d'une liste de termes filtrés et normalisés. On pourra pour cela ré-utiliser les fonctions écrites dans le Lab 1 et fournies dans le fichier [Lab1.py](./Utils/Lab1.py) du répertoire [Utils](./Utils).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "error",
     "timestamp": 1558305023063,
     "user": {
      "displayName": "Tami Myriam",
      "photoUrl": "",
      "userId": "04534705195482337784"
     },
     "user_tz": -120
    },
    "id": "RjdXv0DeNzST",
    "outputId": "87916c9b-eed3-417c-cd35-231f8d037270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KENNEDY', 'ADMINISTRATION', 'PRESSURE', 'NGO', 'DINH', 'DIEM', 'STOP']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils.Lab1 import *\n",
    "\n",
    "def pre_processed_query(query):\n",
    "    tokenized_query = article_tokenize_other(query)\n",
    "    filtered_query = remove_stop_words({\"query\":tokenized_query},load_stop_word(\"./Data/Time/TIME.STP\"))\n",
    "    normalized_query = collection_lemmatize(filtered_query)\n",
    "    return normalized_query[\"query\"]\n",
    "\n",
    "pre_processed_query(\"KENNEDY ADMINISTRATION PRESSURE ON NGO DINH DIEM TO STOP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MACFaqA8NzSW"
   },
   "source": [
    "Un choix doit cependant être fait sur la manière de pondérer les différents termes de la requête et des documents lors de la construction de leurs représentations. En particulier, différents schémas de pondération sont possibles.\n",
    "Il peut être intéressant de pouvoir comparer ces différents types de pondération ainsi, dans la suite nous nous intéresserons aux différents schémas de pondérations suivants :\n",
    "+ **Pondération binaire** comme dans le modèle booléen ;\n",
    "+ **Pondération `tf`** avec donc $w_{id} = tf_{t_i,d}$ où $d$ peut être un document ou une requête ;\n",
    "+ **Pondération `tf-idf`** avec donc $w_{id} = tf_{t_i,d} \\times idf_{t_i}$ où $idf_{t_i} = \\log{\\frac{N}{df_{t_i}}}$ et $N$ étant le nombre de documents dans la collection ;\n",
    "+ **Pondération fréquentielle normalisée** qui consiste à normaliser les fréquences des termes apparaissant dans un document $d$ par la fréquence maximale dans le document $tf_{max_d} = \\max_{t \\in d} tf_{t,d}$ soit pour le terme $p_{tf_{t_i,d}}$ correspondant à la fréquence dans la collection la formule :\n",
    "$ p_{tf_{t_i,d}} = 0.5 + 0.5 \\times \\frac{tf_{t_i,d}}{tf_{max_d}}$ ;\n",
    "+ **Pondération logarithmique des termes** avec $p_{tf_{t_i,d}} = 1 + \\log{tf_{t_i,d}}$ si $tf_{t_i,d} > 0$ et $0$ sinon ;\n",
    "+ **Pondération logarithmique des termes normalisée** qui consiste à normaliser la caractéristique logarithmique précédente par une quantité dépendant de la moyenne $moy_{tf_d} = \\sum_{i=1}^{V}\\frac{tf_{t_i,d}}{|d|}$ ($|d|$ est le nombre de termes uniques du document $d$) des caractéristiques fréquentielles d'un document $d$ soit $ p_{tf_{t_i,d}} = \\frac{1 + \\log{tf_{t_i,d}}}{1 + \\log{moy_{tf_d}}}$ ;\n",
    "+ Et bien entendu, pour chacune de ces différentes pondérations fréquentielles des termes, il est possible de multiplier ou non par le poids du terme dans le collection $p_{df_{t_i}}$ qui peut être soit $idf_{t_i}$ soit une version normalisée $ max(0, \\log{\\frac{N -df_{t_i}}{df_{t_i}}})$.\n",
    "\n",
    "Ces schémas de pondération peuvent être appliqués au document et à la requête. Votre premier travail consiste donc à écrire les fonctions permettant de calculer à partir de l'index inversé et d'informations statistiques additionnelles sur la collection ces différents schémas de pondération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2TaPP-nNzSY"
   },
   "source": [
    "9- Ecrire une fonction `get_tf (term,doc_ID, index_frequence)` qui permet de récupérer la fréquence d'un terme dans un document à partir d'un index de fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3oFIc_oTNzSY",
    "outputId": "db49be24-0dd9-4dcf-9cc5-26dda4caa9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils.saveandload_pickle import *\n",
    "\n",
    "frequence_index = load_inverted_index_pickle('./Index/index_frequence.pickle')\n",
    "\n",
    "def get_tf(term,doc_ID,index_frequence):\n",
    "    return index_frequence[term][doc_ID]\n",
    "\n",
    "get_tf(\"NASSAU\", '017',frequence_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3nR9YeGNzSa"
   },
   "source": [
    "10- Ecrire une fonction `get_tf_logarithmique (term,doc_ID, index_frequence)` qui permet de calculer la pondération logarithmique d'un terme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isIzHW6YNzSa"
   },
   "outputs": [],
   "source": [
    "def get_tf_logarithmique (term,doc_ID, index_frequence):\n",
    "    tf = get_tf(term,doc_ID, index_frequence)\n",
    "    if tf > 0:\n",
    "        return 1 +log(tf)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otq-H4P1NzSc"
   },
   "source": [
    "Pour les autres schémas de pondération, il est nécessaire d'avoir au préalable calculé et stocké plusieurs informations statistiques sur la collection comme le nombre total de documents et pour chaque document, la fréquence maximale dans le document, le nombre de termes uniques dans le document ou encore la moyenne des fréquences dans un document. \n",
    "\n",
    "11- Ecrire une fonction `get_stats_document(document)` qui pour un document donné sous forme d'une liste de tokens renvoie un dictionnaire avec les informations statistiques mentionnées ci-dessus (clés : \"freq_max\", \"unique_terms\", \"freq_moy\"). On pourra utiliser la collection `Counter` du module `Collections` de la bibliothèque standard de python comme dans le LAB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZYYpR7eNzSc",
    "outputId": "c2ee8e24-953e-4a40-8b50-eef9bea66324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freq_max': 6, 'unique_terms': 52, 'freq_moy': 1.4423076923076923}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_stats_document(document):\n",
    "    counter= Counter()\n",
    "    for term in document:\n",
    "        counter.update([term])\n",
    "    stats={}\n",
    "    stats[\"freq_max\"] = counter.most_common(1)[0][1]\n",
    "    stats[\"unique_terms\"] = len(counter.items())\n",
    "    tf_moy = sum(counter.values())\n",
    "    stats[\"freq_moy\"] = tf_moy/len(counter.items())\n",
    "    return stats\n",
    "\n",
    "text = 'UNITED NATIONS POTENT PYGMY IN MANHATTAN LAST WEEK, THE U.N .GREW TO A TOTAL OF 113 MEMBERS WITH THE ADMISSION OF THE NEWLYINDEPENDENT STATES OF ZANZIBAR AND KENYA . ZANZIBAR CONSISTS OF TWOSMALL ISLANDS IN THE INDIAN OCEAN, WITH A TOTAL POPULATION OF 310,000,OR ABOUT THAT OF OMAHA . NEVERTHELESS, ZANZIBAR HAS ONE VOTE IN THEGENERAL ASSEMBLY, AND IS THUS EQUAL IN VOTING POWER WITH SUCH NUCLEARGIANTS AS THE SOVIET UNION AND THE U.S .'\n",
    "document = text.split()\n",
    "\n",
    "print(get_stats_document(document))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SECQQUjVNzSg"
   },
   "source": [
    "12- Ecrire une fonction `get_stats_collection(collection)` qui permet de calculer des informations statistiques sur la collection comme le nombre de documents et pour chaque document de la collection, les informations statistiques précédentes.\n",
    "La fonction retournera un dictionnaire avec les clés \"nb_documents\" et les doc_ID des documents de la collection. On supposera que la collection passée en paramètre est une collection segmentée et pré-traitée (filtrage et normalisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXr_N2yvNzSg"
   },
   "outputs": [],
   "source": [
    "def get_stats_collection(collection):\n",
    "    stats={}\n",
    "    stats[\"nb_docs\"]=len(collection.keys())\n",
    "    for doc in collection:\n",
    "        stats[doc] = get_stats_document(collection[doc])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKMx9ckxNzSi"
   },
   "source": [
    "13- Appliquer cette fonction à la collection TIME. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCdFcLtYNzSi",
    "outputId": "f8c67414-8805-4b73-c5a8-66decc0ef8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 articles ont été parsés\n",
      "{'nb_docs': 423, '017': {'freq_max': 52, 'unique_terms': 417, 'freq_moy': 2.146282973621103}, '018': {'freq_max': 12, 'unique_terms': 143, 'freq_moy': 1.4475524475524475}, '019': {'freq_max': 46, 'unique_terms': 271, 'freq_moy': 1.7933579335793357}, '020': {'freq_max': 9, 'unique_terms': 123, 'freq_moy': 1.3739837398373984}, '021': {'freq_max': 51, 'unique_terms': 416, 'freq_moy': 1.7427884615384615}, '023': {'freq_max': 30, 'unique_terms': 289, 'freq_moy': 1.671280276816609}, '024': {'freq_max': 55, 'unique_terms': 395, 'freq_moy': 1.9367088607594938}, '025': {'freq_max': 7, 'unique_terms': 109, 'freq_moy': 1.4311926605504588}, '026': {'freq_max': 38, 'unique_terms': 242, 'freq_moy': 1.7479338842975207}, '027': {'freq_max': 16, 'unique_terms': 171, 'freq_moy': 1.5847953216374269}, '028': {'freq_max': 51, 'unique_terms': 383, 'freq_moy': 1.783289817232376}, '029': {'freq_max': 74, 'unique_terms': 404, 'freq_moy': 1.9925742574257426}, '030': {'freq_max': 19, 'unique_terms': 232, 'freq_moy': 1.418103448275862}, '031': {'freq_max': 23, 'unique_terms': 311, 'freq_moy': 1.594855305466238}, '032': {'freq_max': 16, 'unique_terms': 162, 'freq_moy': 1.4135802469135803}, '033': {'freq_max': 19, 'unique_terms': 193, 'freq_moy': 1.4922279792746114}, '034': {'freq_max': 35, 'unique_terms': 235, 'freq_moy': 1.6638297872340426}, '035': {'freq_max': 16, 'unique_terms': 188, 'freq_moy': 1.5585106382978724}, '036': {'freq_max': 20, 'unique_terms': 152, 'freq_moy': 1.4473684210526316}, '040': {'freq_max': 46, 'unique_terms': 376, 'freq_moy': 1.8882978723404256}, '042': {'freq_max': 21, 'unique_terms': 250, 'freq_moy': 1.568}, '043': {'freq_max': 21, 'unique_terms': 186, 'freq_moy': 1.60752688172043}, '045': {'freq_max': 24, 'unique_terms': 178, 'freq_moy': 1.6629213483146068}, '047': {'freq_max': 13, 'unique_terms': 147, 'freq_moy': 1.5850340136054422}, '048': {'freq_max': 49, 'unique_terms': 406, 'freq_moy': 1.8300492610837438}, '049': {'freq_max': 21, 'unique_terms': 194, 'freq_moy': 1.577319587628866}, '050': {'freq_max': 30, 'unique_terms': 388, 'freq_moy': 1.6237113402061856}, '051': {'freq_max': 12, 'unique_terms': 168, 'freq_moy': 1.3988095238095237}, '052': {'freq_max': 11, 'unique_terms': 144, 'freq_moy': 1.4305555555555556}, '053': {'freq_max': 23, 'unique_terms': 237, 'freq_moy': 1.6624472573839661}, '054': {'freq_max': 12, 'unique_terms': 158, 'freq_moy': 1.5}, '055': {'freq_max': 93, 'unique_terms': 725, 'freq_moy': 1.913103448275862}, '057': {'freq_max': 35, 'unique_terms': 358, 'freq_moy': 1.6145251396648044}, '058': {'freq_max': 39, 'unique_terms': 368, 'freq_moy': 1.747282608695652}, '059': {'freq_max': 14, 'unique_terms': 205, 'freq_moy': 1.5219512195121951}, '060': {'freq_max': 14, 'unique_terms': 134, 'freq_moy': 1.4328358208955223}, '061': {'freq_max': 11, 'unique_terms': 163, 'freq_moy': 1.4294478527607362}, '062': {'freq_max': 40, 'unique_terms': 395, 'freq_moy': 1.8481012658227849}, '063': {'freq_max': 34, 'unique_terms': 352, 'freq_moy': 1.7556818181818181}, '064': {'freq_max': 23, 'unique_terms': 196, 'freq_moy': 1.760204081632653}, '065': {'freq_max': 30, 'unique_terms': 258, 'freq_moy': 1.624031007751938}, '066': {'freq_max': 30, 'unique_terms': 210, 'freq_moy': 1.6333333333333333}, '067': {'freq_max': 28, 'unique_terms': 367, 'freq_moy': 1.776566757493188}, '068': {'freq_max': 27, 'unique_terms': 234, 'freq_moy': 1.6025641025641026}, '069': {'freq_max': 36, 'unique_terms': 327, 'freq_moy': 1.5779816513761469}, '070': {'freq_max': 18, 'unique_terms': 166, 'freq_moy': 1.4578313253012047}, '071': {'freq_max': 282, 'unique_terms': 1470, 'freq_moy': 2.831972789115646}, '072': {'freq_max': 96, 'unique_terms': 490, 'freq_moy': 2.289795918367347}, '081': {'freq_max': 33, 'unique_terms': 342, 'freq_moy': 1.8011695906432748}, '082': {'freq_max': 19, 'unique_terms': 176, 'freq_moy': 1.5965909090909092}, '083': {'freq_max': 21, 'unique_terms': 158, 'freq_moy': 1.5949367088607596}, '084': {'freq_max': 22, 'unique_terms': 256, 'freq_moy': 1.44140625}, '085': {'freq_max': 36, 'unique_terms': 334, 'freq_moy': 1.8502994011976048}, '086': {'freq_max': 41, 'unique_terms': 363, 'freq_moy': 1.8898071625344353}, '087': {'freq_max': 61, 'unique_terms': 473, 'freq_moy': 1.9260042283298098}, '088': {'freq_max': 39, 'unique_terms': 326, 'freq_moy': 1.8987730061349692}, '090': {'freq_max': 15, 'unique_terms': 263, 'freq_moy': 1.6653992395437263}, '091': {'freq_max': 13, 'unique_terms': 156, 'freq_moy': 1.5769230769230769}, '092': {'freq_max': 15, 'unique_terms': 194, 'freq_moy': 1.597938144329897}, '093': {'freq_max': 16, 'unique_terms': 207, 'freq_moy': 1.5072463768115942}, '094': {'freq_max': 24, 'unique_terms': 233, 'freq_moy': 1.6437768240343347}, '095': {'freq_max': 26, 'unique_terms': 217, 'freq_moy': 1.5391705069124424}, '096': {'freq_max': 20, 'unique_terms': 131, 'freq_moy': 1.633587786259542}, '097': {'freq_max': 47, 'unique_terms': 293, 'freq_moy': 1.8293515358361774}, '098': {'freq_max': 43, 'unique_terms': 355, 'freq_moy': 1.7943661971830986}, '099': {'freq_max': 25, 'unique_terms': 202, 'freq_moy': 1.5}, '100': {'freq_max': 18, 'unique_terms': 183, 'freq_moy': 1.53551912568306}, '101': {'freq_max': 23, 'unique_terms': 146, 'freq_moy': 1.4794520547945205}, '102': {'freq_max': 91, 'unique_terms': 531, 'freq_moy': 2.056497175141243}, '104': {'freq_max': 66, 'unique_terms': 431, 'freq_moy': 1.863109048723898}, '105': {'freq_max': 67, 'unique_terms': 511, 'freq_moy': 1.9706457925636007}, '106': {'freq_max': 26, 'unique_terms': 273, 'freq_moy': 1.47985347985348}, '107': {'freq_max': 20, 'unique_terms': 205, 'freq_moy': 1.4048780487804877}, '108': {'freq_max': 14, 'unique_terms': 173, 'freq_moy': 1.4624277456647399}, '109': {'freq_max': 15, 'unique_terms': 191, 'freq_moy': 1.5549738219895288}, '110': {'freq_max': 31, 'unique_terms': 294, 'freq_moy': 1.5748299319727892}, '111': {'freq_max': 17, 'unique_terms': 230, 'freq_moy': 1.6434782608695653}, '112': {'freq_max': 18, 'unique_terms': 221, 'freq_moy': 1.588235294117647}, '113': {'freq_max': 73, 'unique_terms': 567, 'freq_moy': 1.8324514991181657}, '115': {'freq_max': 18, 'unique_terms': 137, 'freq_moy': 1.5985401459854014}, '116': {'freq_max': 26, 'unique_terms': 213, 'freq_moy': 1.7089201877934272}, '117': {'freq_max': 19, 'unique_terms': 177, 'freq_moy': 1.4519774011299436}, '118': {'freq_max': 41, 'unique_terms': 241, 'freq_moy': 1.95850622406639}, '119': {'freq_max': 16, 'unique_terms': 170, 'freq_moy': 1.464705882352941}, '120': {'freq_max': 18, 'unique_terms': 160, 'freq_moy': 1.4625}, '121': {'freq_max': 41, 'unique_terms': 350, 'freq_moy': 1.7342857142857142}, '122': {'freq_max': 30, 'unique_terms': 229, 'freq_moy': 1.6899563318777293}, '123': {'freq_max': 78, 'unique_terms': 574, 'freq_moy': 1.9529616724738676}, '126': {'freq_max': 55, 'unique_terms': 410, 'freq_moy': 1.951219512195122}, '128': {'freq_max': 21, 'unique_terms': 256, 'freq_moy': 1.6328125}, '129': {'freq_max': 56, 'unique_terms': 352, 'freq_moy': 1.8948863636363635}, '130': {'freq_max': 18, 'unique_terms': 214, 'freq_moy': 1.6869158878504673}, '131': {'freq_max': 10, 'unique_terms': 230, 'freq_moy': 1.491304347826087}, '133': {'freq_max': 28, 'unique_terms': 297, 'freq_moy': 1.6632996632996633}, '134': {'freq_max': 12, 'unique_terms': 170, 'freq_moy': 1.4588235294117646}, '135': {'freq_max': 22, 'unique_terms': 207, 'freq_moy': 1.7246376811594204}, '136': {'freq_max': 27, 'unique_terms': 316, 'freq_moy': 1.6708860759493671}, '137': {'freq_max': 21, 'unique_terms': 302, 'freq_moy': 1.5695364238410596}, '138': {'freq_max': 89, 'unique_terms': 628, 'freq_moy': 2.111464968152866}, '140': {'freq_max': 90, 'unique_terms': 588, 'freq_moy': 2.003401360544218}, '143': {'freq_max': 36, 'unique_terms': 249, 'freq_moy': 1.7028112449799198}, '144': {'freq_max': 6, 'unique_terms': 134, 'freq_moy': 1.3432835820895523}, '145': {'freq_max': 15, 'unique_terms': 210, 'freq_moy': 1.4761904761904763}, '146': {'freq_max': 27, 'unique_terms': 251, 'freq_moy': 1.6015936254980079}, '147': {'freq_max': 21, 'unique_terms': 196, 'freq_moy': 1.469387755102041}, '148': {'freq_max': 18, 'unique_terms': 171, 'freq_moy': 1.5847953216374269}, '149': {'freq_max': 31, 'unique_terms': 308, 'freq_moy': 1.7662337662337662}, '150': {'freq_max': 21, 'unique_terms': 254, 'freq_moy': 1.6968503937007875}, '151': {'freq_max': 7, 'unique_terms': 105, 'freq_moy': 1.2476190476190476}, '152': {'freq_max': 57, 'unique_terms': 430, 'freq_moy': 1.7906976744186047}, '153': {'freq_max': 14, 'unique_terms': 125, 'freq_moy': 1.392}, '154': {'freq_max': 11, 'unique_terms': 137, 'freq_moy': 1.3941605839416058}, '155': {'freq_max': 55, 'unique_terms': 465, 'freq_moy': 1.864516129032258}, '156': {'freq_max': 16, 'unique_terms': 132, 'freq_moy': 1.5454545454545454}, '157': {'freq_max': 41, 'unique_terms': 442, 'freq_moy': 1.7805429864253393}, '158': {'freq_max': 34, 'unique_terms': 329, 'freq_moy': 1.7264437689969605}, '159': {'freq_max': 32, 'unique_terms': 368, 'freq_moy': 1.673913043478261}, '160': {'freq_max': 10, 'unique_terms': 129, 'freq_moy': 1.4108527131782946}, '161': {'freq_max': 30, 'unique_terms': 248, 'freq_moy': 1.7419354838709677}, '162': {'freq_max': 41, 'unique_terms': 303, 'freq_moy': 1.5313531353135315}, '163': {'freq_max': 349, 'unique_terms': 1806, 'freq_moy': 2.7602436323366555}, '170': {'freq_max': 40, 'unique_terms': 383, 'freq_moy': 1.9425587467362924}, '171': {'freq_max': 11, 'unique_terms': 179, 'freq_moy': 1.3966480446927374}, '172': {'freq_max': 8, 'unique_terms': 145, 'freq_moy': 1.393103448275862}, '173': {'freq_max': 12, 'unique_terms': 149, 'freq_moy': 1.4966442953020134}, '174': {'freq_max': 35, 'unique_terms': 307, 'freq_moy': 1.713355048859935}, '175': {'freq_max': 28, 'unique_terms': 224, 'freq_moy': 1.5669642857142858}, '176': {'freq_max': 22, 'unique_terms': 188, 'freq_moy': 1.5585106382978724}, '177': {'freq_max': 14, 'unique_terms': 162, 'freq_moy': 1.4197530864197532}, '178': {'freq_max': 23, 'unique_terms': 182, 'freq_moy': 1.489010989010989}, '179': {'freq_max': 34, 'unique_terms': 317, 'freq_moy': 1.5173501577287065}, '180': {'freq_max': 13, 'unique_terms': 208, 'freq_moy': 1.5192307692307692}, '181': {'freq_max': 29, 'unique_terms': 246, 'freq_moy': 1.6382113821138211}, '182': {'freq_max': 37, 'unique_terms': 208, 'freq_moy': 1.7355769230769231}, '183': {'freq_max': 40, 'unique_terms': 291, 'freq_moy': 2.0240549828178693}, '184': {'freq_max': 21, 'unique_terms': 225, 'freq_moy': 1.5333333333333334}, '185': {'freq_max': 14, 'unique_terms': 148, 'freq_moy': 1.5135135135135136}, '186': {'freq_max': 29, 'unique_terms': 334, 'freq_moy': 1.6317365269461077}, '187': {'freq_max': 24, 'unique_terms': 285, 'freq_moy': 1.712280701754386}, '188': {'freq_max': 34, 'unique_terms': 311, 'freq_moy': 1.7588424437299035}, '189': {'freq_max': 31, 'unique_terms': 241, 'freq_moy': 1.4813278008298756}, '190': {'freq_max': 42, 'unique_terms': 357, 'freq_moy': 1.7002801120448179}, '191': {'freq_max': 27, 'unique_terms': 207, 'freq_moy': 1.816425120772947}, '192': {'freq_max': 23, 'unique_terms': 251, 'freq_moy': 1.8047808764940239}, '193': {'freq_max': 15, 'unique_terms': 199, 'freq_moy': 1.4321608040201006}, '194': {'freq_max': 26, 'unique_terms': 290, 'freq_moy': 1.6413793103448275}, '195': {'freq_max': 46, 'unique_terms': 304, 'freq_moy': 1.9605263157894737}, '196': {'freq_max': 30, 'unique_terms': 335, 'freq_moy': 1.8059701492537314}, '197': {'freq_max': 16, 'unique_terms': 179, 'freq_moy': 1.4357541899441342}, '198': {'freq_max': 29, 'unique_terms': 187, 'freq_moy': 1.6844919786096257}, '199': {'freq_max': 14, 'unique_terms': 125, 'freq_moy': 1.512}, '200': {'freq_max': 44, 'unique_terms': 357, 'freq_moy': 1.69187675070028}, '201': {'freq_max': 22, 'unique_terms': 179, 'freq_moy': 1.6536312849162011}, '202': {'freq_max': 7, 'unique_terms': 105, 'freq_moy': 1.4857142857142858}, '203': {'freq_max': 48, 'unique_terms': 473, 'freq_moy': 1.8710359408033828}, '204': {'freq_max': 242, 'unique_terms': 1267, 'freq_moy': 2.4814522494080506}, '213': {'freq_max': 26, 'unique_terms': 337, 'freq_moy': 1.801186943620178}, '214': {'freq_max': 19, 'unique_terms': 173, 'freq_moy': 1.4971098265895955}, '215': {'freq_max': 76, 'unique_terms': 534, 'freq_moy': 1.895131086142322}, '217': {'freq_max': 45, 'unique_terms': 405, 'freq_moy': 1.6864197530864198}, '218': {'freq_max': 12, 'unique_terms': 97, 'freq_moy': 1.3298969072164948}, '219': {'freq_max': 16, 'unique_terms': 218, 'freq_moy': 1.4954128440366972}, '220': {'freq_max': 51, 'unique_terms': 305, 'freq_moy': 1.780327868852459}, '221': {'freq_max': 21, 'unique_terms': 206, 'freq_moy': 1.6213592233009708}, '222': {'freq_max': 24, 'unique_terms': 221, 'freq_moy': 1.6018099547511313}, '223': {'freq_max': 33, 'unique_terms': 282, 'freq_moy': 1.5780141843971631}, '224': {'freq_max': 29, 'unique_terms': 232, 'freq_moy': 1.6982758620689655}, '225': {'freq_max': 17, 'unique_terms': 234, 'freq_moy': 1.5427350427350428}, '226': {'freq_max': 52, 'unique_terms': 330, 'freq_moy': 2.006060606060606}, '227': {'freq_max': 57, 'unique_terms': 374, 'freq_moy': 1.8716577540106951}, '228': {'freq_max': 31, 'unique_terms': 225, 'freq_moy': 1.6533333333333333}, '229': {'freq_max': 5, 'unique_terms': 61, 'freq_moy': 1.180327868852459}, '230': {'freq_max': 47, 'unique_terms': 300, 'freq_moy': 1.73}, '231': {'freq_max': 25, 'unique_terms': 217, 'freq_moy': 1.6451612903225807}, '232': {'freq_max': 75, 'unique_terms': 660, 'freq_moy': 1.8515151515151516}, '234': {'freq_max': 30, 'unique_terms': 275, 'freq_moy': 1.5454545454545454}, '235': {'freq_max': 44, 'unique_terms': 322, 'freq_moy': 1.7298136645962734}, '236': {'freq_max': 25, 'unique_terms': 237, 'freq_moy': 1.620253164556962}, '237': {'freq_max': 21, 'unique_terms': 224, 'freq_moy': 1.5178571428571428}, '238': {'freq_max': 19, 'unique_terms': 183, 'freq_moy': 1.3825136612021858}, '239': {'freq_max': 12, 'unique_terms': 121, 'freq_moy': 1.4793388429752066}, '240': {'freq_max': 44, 'unique_terms': 323, 'freq_moy': 1.761609907120743}, '241': {'freq_max': 21, 'unique_terms': 244, 'freq_moy': 1.6229508196721312}, '242': {'freq_max': 9, 'unique_terms': 103, 'freq_moy': 1.5145631067961165}, '243': {'freq_max': 87, 'unique_terms': 456, 'freq_moy': 1.9714912280701755}, '244': {'freq_max': 39, 'unique_terms': 312, 'freq_moy': 1.7115384615384615}, '245': {'freq_max': 4, 'unique_terms': 63, 'freq_moy': 1.2698412698412698}, '246': {'freq_max': 65, 'unique_terms': 565, 'freq_moy': 1.9150442477876106}, '247': {'freq_max': 48, 'unique_terms': 329, 'freq_moy': 1.750759878419453}, '248': {'freq_max': 6, 'unique_terms': 81, 'freq_moy': 1.2962962962962963}, '249': {'freq_max': 15, 'unique_terms': 193, 'freq_moy': 1.4455958549222798}, '250': {'freq_max': 23, 'unique_terms': 224, 'freq_moy': 1.7053571428571428}, '251': {'freq_max': 39, 'unique_terms': 423, 'freq_moy': 1.6855791962174942}, '252': {'freq_max': 42, 'unique_terms': 415, 'freq_moy': 1.8433734939759037}, '253': {'freq_max': 38, 'unique_terms': 310, 'freq_moy': 1.7032258064516128}, '254': {'freq_max': 34, 'unique_terms': 363, 'freq_moy': 1.6887052341597797}, '255': {'freq_max': 18, 'unique_terms': 129, 'freq_moy': 1.4651162790697674}, '256': {'freq_max': 13, 'unique_terms': 132, 'freq_moy': 1.4545454545454546}, '257': {'freq_max': 27, 'unique_terms': 200, 'freq_moy': 1.515}, '258': {'freq_max': 15, 'unique_terms': 165, 'freq_moy': 1.3515151515151516}, '259': {'freq_max': 15, 'unique_terms': 199, 'freq_moy': 1.3768844221105527}, '260': {'freq_max': 45, 'unique_terms': 364, 'freq_moy': 2.0247252747252746}, '261': {'freq_max': 15, 'unique_terms': 186, 'freq_moy': 1.4731182795698925}, '262': {'freq_max': 41, 'unique_terms': 414, 'freq_moy': 1.9275362318840579}, '263': {'freq_max': 16, 'unique_terms': 172, 'freq_moy': 1.569767441860465}, '264': {'freq_max': 31, 'unique_terms': 291, 'freq_moy': 1.7938144329896908}, '265': {'freq_max': 14, 'unique_terms': 197, 'freq_moy': 1.5431472081218274}, '266': {'freq_max': 32, 'unique_terms': 315, 'freq_moy': 1.692063492063492}, '267': {'freq_max': 16, 'unique_terms': 255, 'freq_moy': 1.6588235294117648}, '268': {'freq_max': 23, 'unique_terms': 199, 'freq_moy': 1.6532663316582914}, '269': {'freq_max': 113, 'unique_terms': 684, 'freq_moy': 2.2953216374269005}, '270': {'freq_max': 41, 'unique_terms': 381, 'freq_moy': 1.8661417322834646}, '272': {'freq_max': 16, 'unique_terms': 156, 'freq_moy': 1.544871794871795}, '273': {'freq_max': 20, 'unique_terms': 229, 'freq_moy': 1.6026200873362446}, '274': {'freq_max': 36, 'unique_terms': 333, 'freq_moy': 1.6846846846846846}, '275': {'freq_max': 18, 'unique_terms': 152, 'freq_moy': 1.4210526315789473}, '276': {'freq_max': 16, 'unique_terms': 183, 'freq_moy': 1.4262295081967213}, '277': {'freq_max': 16, 'unique_terms': 183, 'freq_moy': 1.540983606557377}, '278': {'freq_max': 39, 'unique_terms': 290, 'freq_moy': 1.8655172413793104}, '279': {'freq_max': 13, 'unique_terms': 123, 'freq_moy': 1.5121951219512195}, '280': {'freq_max': 51, 'unique_terms': 440, 'freq_moy': 1.768181818181818}, '281': {'freq_max': 22, 'unique_terms': 202, 'freq_moy': 1.5396039603960396}, '282': {'freq_max': 16, 'unique_terms': 181, 'freq_moy': 1.558011049723757}, '283': {'freq_max': 18, 'unique_terms': 187, 'freq_moy': 1.5240641711229947}, '284': {'freq_max': 21, 'unique_terms': 222, 'freq_moy': 1.4684684684684686}, '285': {'freq_max': 17, 'unique_terms': 257, 'freq_moy': 1.5719844357976653}, '286': {'freq_max': 22, 'unique_terms': 261, 'freq_moy': 1.5402298850574712}, '287': {'freq_max': 48, 'unique_terms': 339, 'freq_moy': 1.9970501474926254}, '288': {'freq_max': 32, 'unique_terms': 305, 'freq_moy': 1.6360655737704919}, '289': {'freq_max': 28, 'unique_terms': 353, 'freq_moy': 1.5439093484419264}, '292': {'freq_max': 26, 'unique_terms': 265, 'freq_moy': 1.539622641509434}, '293': {'freq_max': 12, 'unique_terms': 194, 'freq_moy': 1.5051546391752577}, '294': {'freq_max': 68, 'unique_terms': 402, 'freq_moy': 1.855721393034826}, '295': {'freq_max': 24, 'unique_terms': 244, 'freq_moy': 1.6229508196721312}, '296': {'freq_max': 28, 'unique_terms': 220, 'freq_moy': 1.709090909090909}, '297': {'freq_max': 56, 'unique_terms': 470, 'freq_moy': 1.7680851063829788}, '298': {'freq_max': 18, 'unique_terms': 171, 'freq_moy': 1.6198830409356726}, '299': {'freq_max': 26, 'unique_terms': 343, 'freq_moy': 1.6355685131195334}, '300': {'freq_max': 29, 'unique_terms': 155, 'freq_moy': 1.6774193548387097}, '301': {'freq_max': 35, 'unique_terms': 328, 'freq_moy': 1.954268292682927}, '302': {'freq_max': 27, 'unique_terms': 297, 'freq_moy': 1.7575757575757576}, '303': {'freq_max': 13, 'unique_terms': 165, 'freq_moy': 1.5212121212121212}, '304': {'freq_max': 20, 'unique_terms': 173, 'freq_moy': 1.583815028901734}, '305': {'freq_max': 26, 'unique_terms': 251, 'freq_moy': 1.5856573705179282}, '306': {'freq_max': 33, 'unique_terms': 243, 'freq_moy': 1.625514403292181}, '307': {'freq_max': 21, 'unique_terms': 234, 'freq_moy': 1.5769230769230769}, '308': {'freq_max': 23, 'unique_terms': 280, 'freq_moy': 1.7392857142857143}, '309': {'freq_max': 23, 'unique_terms': 288, 'freq_moy': 1.5104166666666667}, '310': {'freq_max': 23, 'unique_terms': 301, 'freq_moy': 1.4285714285714286}, '311': {'freq_max': 21, 'unique_terms': 268, 'freq_moy': 1.544776119402985}, '312': {'freq_max': 27, 'unique_terms': 193, 'freq_moy': 1.7409326424870466}, '313': {'freq_max': 92, 'unique_terms': 593, 'freq_moy': 2.0320404721753795}, '315': {'freq_max': 64, 'unique_terms': 598, 'freq_moy': 2.0903010033444818}, '317': {'freq_max': 30, 'unique_terms': 262, 'freq_moy': 1.6984732824427482}, '318': {'freq_max': 65, 'unique_terms': 455, 'freq_moy': 1.9230769230769231}, '319': {'freq_max': 37, 'unique_terms': 379, 'freq_moy': 1.562005277044855}, '320': {'freq_max': 63, 'unique_terms': 368, 'freq_moy': 1.877717391304348}, '321': {'freq_max': 15, 'unique_terms': 231, 'freq_moy': 1.4891774891774892}, '322': {'freq_max': 22, 'unique_terms': 230, 'freq_moy': 1.4869565217391305}, '323': {'freq_max': 18, 'unique_terms': 245, 'freq_moy': 1.453061224489796}, '324': {'freq_max': 65, 'unique_terms': 523, 'freq_moy': 1.965583173996176}, '326': {'freq_max': 64, 'unique_terms': 566, 'freq_moy': 1.9081272084805654}, '329': {'freq_max': 32, 'unique_terms': 280, 'freq_moy': 1.6821428571428572}, '330': {'freq_max': 40, 'unique_terms': 364, 'freq_moy': 1.9120879120879122}, '331': {'freq_max': 21, 'unique_terms': 195, 'freq_moy': 1.5435897435897437}, '332': {'freq_max': 46, 'unique_terms': 398, 'freq_moy': 1.6909547738693467}, '333': {'freq_max': 41, 'unique_terms': 321, 'freq_moy': 1.6728971962616823}, '334': {'freq_max': 41, 'unique_terms': 288, 'freq_moy': 1.7291666666666667}, '335': {'freq_max': 18, 'unique_terms': 174, 'freq_moy': 1.6091954022988506}, '336': {'freq_max': 54, 'unique_terms': 478, 'freq_moy': 1.8389121338912133}, '337': {'freq_max': 150, 'unique_terms': 992, 'freq_moy': 2.3911290322580645}, '341': {'freq_max': 47, 'unique_terms': 494, 'freq_moy': 1.7955465587044535}, '342': {'freq_max': 38, 'unique_terms': 456, 'freq_moy': 2.0592105263157894}, '345': {'freq_max': 43, 'unique_terms': 454, 'freq_moy': 1.8215859030837005}, '346': {'freq_max': 37, 'unique_terms': 341, 'freq_moy': 1.6979472140762464}, '347': {'freq_max': 14, 'unique_terms': 122, 'freq_moy': 1.3934426229508197}, '348': {'freq_max': 66, 'unique_terms': 441, 'freq_moy': 1.8934240362811792}, '350': {'freq_max': 20, 'unique_terms': 301, 'freq_moy': 1.691029900332226}, '351': {'freq_max': 56, 'unique_terms': 452, 'freq_moy': 1.8849557522123894}, '353': {'freq_max': 67, 'unique_terms': 491, 'freq_moy': 1.7311608961303462}, '354': {'freq_max': 54, 'unique_terms': 451, 'freq_moy': 2.0953436807095343}, '355': {'freq_max': 14, 'unique_terms': 177, 'freq_moy': 1.3785310734463276}, '356': {'freq_max': 20, 'unique_terms': 283, 'freq_moy': 1.6784452296819787}, '357': {'freq_max': 75, 'unique_terms': 485, 'freq_moy': 1.975257731958763}, '358': {'freq_max': 25, 'unique_terms': 190, 'freq_moy': 1.6105263157894736}, '359': {'freq_max': 44, 'unique_terms': 352, 'freq_moy': 1.6448863636363635}, '361': {'freq_max': 53, 'unique_terms': 439, 'freq_moy': 1.8405466970387243}, '363': {'freq_max': 35, 'unique_terms': 286, 'freq_moy': 1.5314685314685315}, '364': {'freq_max': 45, 'unique_terms': 365, 'freq_moy': 1.747945205479452}, '365': {'freq_max': 99, 'unique_terms': 787, 'freq_moy': 2.1499364675984753}, '367': {'freq_max': 22, 'unique_terms': 203, 'freq_moy': 1.6502463054187193}, '368': {'freq_max': 56, 'unique_terms': 407, 'freq_moy': 1.9582309582309583}, '369': {'freq_max': 49, 'unique_terms': 366, 'freq_moy': 1.9262295081967213}, '370': {'freq_max': 257, 'unique_terms': 1838, 'freq_moy': 2.44341675734494}, '380': {'freq_max': 43, 'unique_terms': 359, 'freq_moy': 2.0167130919220058}, '381': {'freq_max': 63, 'unique_terms': 359, 'freq_moy': 1.818941504178273}, '382': {'freq_max': 42, 'unique_terms': 409, 'freq_moy': 1.706601466992665}, '383': {'freq_max': 55, 'unique_terms': 438, 'freq_moy': 1.9406392694063928}, '384': {'freq_max': 10, 'unique_terms': 158, 'freq_moy': 1.4177215189873418}, '385': {'freq_max': 37, 'unique_terms': 515, 'freq_moy': 1.5922330097087378}, '386': {'freq_max': 61, 'unique_terms': 529, 'freq_moy': 1.8185255198487713}, '388': {'freq_max': 27, 'unique_terms': 206, 'freq_moy': 1.5388349514563107}, '389': {'freq_max': 36, 'unique_terms': 256, 'freq_moy': 1.80859375}, '390': {'freq_max': 46, 'unique_terms': 384, 'freq_moy': 1.9192708333333333}, '391': {'freq_max': 23, 'unique_terms': 272, 'freq_moy': 1.6654411764705883}, '392': {'freq_max': 65, 'unique_terms': 461, 'freq_moy': 1.9566160520607376}, '394': {'freq_max': 72, 'unique_terms': 571, 'freq_moy': 1.9001751313485113}, '396': {'freq_max': 71, 'unique_terms': 567, 'freq_moy': 1.9347442680776015}, '398': {'freq_max': 30, 'unique_terms': 229, 'freq_moy': 1.5851528384279476}, '399': {'freq_max': 18, 'unique_terms': 159, 'freq_moy': 1.578616352201258}, '400': {'freq_max': 37, 'unique_terms': 498, 'freq_moy': 1.8152610441767068}, '401': {'freq_max': 26, 'unique_terms': 266, 'freq_moy': 1.6428571428571428}, '402': {'freq_max': 32, 'unique_terms': 270, 'freq_moy': 1.5962962962962963}, '403': {'freq_max': 18, 'unique_terms': 181, 'freq_moy': 1.5966850828729282}, '404': {'freq_max': 50, 'unique_terms': 364, 'freq_moy': 1.8379120879120878}, '405': {'freq_max': 46, 'unique_terms': 451, 'freq_moy': 1.9157427937915743}, '406': {'freq_max': 14, 'unique_terms': 99, 'freq_moy': 1.3838383838383839}, '407': {'freq_max': 16, 'unique_terms': 230, 'freq_moy': 1.5130434782608695}, '408': {'freq_max': 47, 'unique_terms': 449, 'freq_moy': 1.7594654788418709}, '411': {'freq_max': 8, 'unique_terms': 140, 'freq_moy': 1.4785714285714286}, '412': {'freq_max': 36, 'unique_terms': 439, 'freq_moy': 1.6674259681093395}, '413': {'freq_max': 23, 'unique_terms': 230, 'freq_moy': 1.6521739130434783}, '414': {'freq_max': 33, 'unique_terms': 297, 'freq_moy': 1.6666666666666667}, '415': {'freq_max': 79, 'unique_terms': 542, 'freq_moy': 1.977859778597786}, '417': {'freq_max': 18, 'unique_terms': 148, 'freq_moy': 1.4527027027027026}, '418': {'freq_max': 207, 'unique_terms': 985, 'freq_moy': 2.4761421319796955}, '422': {'freq_max': 42, 'unique_terms': 502, 'freq_moy': 1.8286852589641434}, '424': {'freq_max': 42, 'unique_terms': 317, 'freq_moy': 1.8832807570977919}, '425': {'freq_max': 39, 'unique_terms': 407, 'freq_moy': 1.8353808353808354}, '426': {'freq_max': 16, 'unique_terms': 217, 'freq_moy': 1.5944700460829493}, '427': {'freq_max': 34, 'unique_terms': 307, 'freq_moy': 1.700325732899023}, '430': {'freq_max': 23, 'unique_terms': 244, 'freq_moy': 1.6147540983606556}, '431': {'freq_max': 67, 'unique_terms': 546, 'freq_moy': 1.8553113553113554}, '434': {'freq_max': 104, 'unique_terms': 729, 'freq_moy': 2.307270233196159}, '436': {'freq_max': 66, 'unique_terms': 503, 'freq_moy': 1.6819085487077534}, '437': {'freq_max': 29, 'unique_terms': 220, 'freq_moy': 1.6}, '438': {'freq_max': 82, 'unique_terms': 544, 'freq_moy': 2.2316176470588234}, '442': {'freq_max': 29, 'unique_terms': 298, 'freq_moy': 1.5738255033557047}, '443': {'freq_max': 32, 'unique_terms': 347, 'freq_moy': 1.717579250720461}, '444': {'freq_max': 39, 'unique_terms': 356, 'freq_moy': 1.6629213483146068}, '445': {'freq_max': 23, 'unique_terms': 253, 'freq_moy': 1.790513833992095}, '446': {'freq_max': 40, 'unique_terms': 220, 'freq_moy': 1.65}, '449': {'freq_max': 313, 'unique_terms': 1725, 'freq_moy': 2.5739130434782607}, '459': {'freq_max': 42, 'unique_terms': 348, 'freq_moy': 1.735632183908046}, '460': {'freq_max': 54, 'unique_terms': 475, 'freq_moy': 1.7663157894736843}, '461': {'freq_max': 18, 'unique_terms': 189, 'freq_moy': 1.560846560846561}, '462': {'freq_max': 21, 'unique_terms': 156, 'freq_moy': 1.4038461538461537}, '463': {'freq_max': 32, 'unique_terms': 295, 'freq_moy': 1.7050847457627119}, '464': {'freq_max': 148, 'unique_terms': 854, 'freq_moy': 2.2728337236533958}, '470': {'freq_max': 37, 'unique_terms': 274, 'freq_moy': 1.6934306569343065}, '471': {'freq_max': 17, 'unique_terms': 204, 'freq_moy': 1.4509803921568627}, '472': {'freq_max': 53, 'unique_terms': 437, 'freq_moy': 2.0068649885583523}, '473': {'freq_max': 33, 'unique_terms': 161, 'freq_moy': 1.68944099378882}, '475': {'freq_max': 25, 'unique_terms': 236, 'freq_moy': 1.6059322033898304}, '476': {'freq_max': 37, 'unique_terms': 299, 'freq_moy': 1.6923076923076923}, '477': {'freq_max': 34, 'unique_terms': 303, 'freq_moy': 1.5643564356435644}, '478': {'freq_max': 38, 'unique_terms': 257, 'freq_moy': 1.7587548638132295}, '479': {'freq_max': 27, 'unique_terms': 237, 'freq_moy': 1.649789029535865}, '480': {'freq_max': 124, 'unique_terms': 867, 'freq_moy': 2.2168396770472896}, '485': {'freq_max': 79, 'unique_terms': 436, 'freq_moy': 2.018348623853211}, '487': {'freq_max': 34, 'unique_terms': 271, 'freq_moy': 1.6383763837638377}, '490': {'freq_max': 10, 'unique_terms': 106, 'freq_moy': 1.2641509433962264}, '491': {'freq_max': 4, 'unique_terms': 110, 'freq_moy': 1.3}, '492': {'freq_max': 20, 'unique_terms': 157, 'freq_moy': 1.624203821656051}, '493': {'freq_max': 23, 'unique_terms': 242, 'freq_moy': 1.5206611570247934}, '494': {'freq_max': 32, 'unique_terms': 341, 'freq_moy': 1.689149560117302}, '495': {'freq_max': 67, 'unique_terms': 564, 'freq_moy': 2.097517730496454}, '496': {'freq_max': 13, 'unique_terms': 130, 'freq_moy': 1.4461538461538461}, '497': {'freq_max': 27, 'unique_terms': 212, 'freq_moy': 1.6367924528301887}, '498': {'freq_max': 134, 'unique_terms': 786, 'freq_moy': 2.199745547073791}, '501': {'freq_max': 39, 'unique_terms': 356, 'freq_moy': 1.6882022471910112}, '502': {'freq_max': 11, 'unique_terms': 130, 'freq_moy': 1.3307692307692307}, '503': {'freq_max': 34, 'unique_terms': 350, 'freq_moy': 1.6742857142857144}, '504': {'freq_max': 19, 'unique_terms': 196, 'freq_moy': 1.4489795918367347}, '507': {'freq_max': 29, 'unique_terms': 217, 'freq_moy': 1.6267281105990783}, '508': {'freq_max': 15, 'unique_terms': 196, 'freq_moy': 1.5510204081632653}, '509': {'freq_max': 105, 'unique_terms': 852, 'freq_moy': 2.1279342723004695}, '511': {'freq_max': 35, 'unique_terms': 333, 'freq_moy': 1.9009009009009008}, '512': {'freq_max': 32, 'unique_terms': 305, 'freq_moy': 1.681967213114754}, '513': {'freq_max': 42, 'unique_terms': 355, 'freq_moy': 1.676056338028169}, '514': {'freq_max': 48, 'unique_terms': 262, 'freq_moy': 1.6908396946564885}, '516': {'freq_max': 82, 'unique_terms': 593, 'freq_moy': 1.9139966273187183}, '518': {'freq_max': 41, 'unique_terms': 289, 'freq_moy': 1.698961937716263}, '519': {'freq_max': 50, 'unique_terms': 380, 'freq_moy': 1.855263157894737}, '521': {'freq_max': 13, 'unique_terms': 130, 'freq_moy': 1.523076923076923}, '522': {'freq_max': 28, 'unique_terms': 210, 'freq_moy': 1.680952380952381}, '523': {'freq_max': 18, 'unique_terms': 187, 'freq_moy': 1.5133689839572193}, '524': {'freq_max': 55, 'unique_terms': 354, 'freq_moy': 1.92090395480226}, '525': {'freq_max': 24, 'unique_terms': 239, 'freq_moy': 1.5439330543933054}, '526': {'freq_max': 17, 'unique_terms': 182, 'freq_moy': 1.456043956043956}, '527': {'freq_max': 23, 'unique_terms': 294, 'freq_moy': 1.6122448979591837}, '528': {'freq_max': 35, 'unique_terms': 266, 'freq_moy': 1.7406015037593985}, '529': {'freq_max': 9, 'unique_terms': 163, 'freq_moy': 1.4110429447852761}, '530': {'freq_max': 54, 'unique_terms': 518, 'freq_moy': 1.9092664092664093}, '533': {'freq_max': 52, 'unique_terms': 438, 'freq_moy': 1.6963470319634704}, '534': {'freq_max': 21, 'unique_terms': 281, 'freq_moy': 1.594306049822064}, '535': {'freq_max': 33, 'unique_terms': 383, 'freq_moy': 1.629242819843342}, '536': {'freq_max': 27, 'unique_terms': 275, 'freq_moy': 1.6581818181818182}, '537': {'freq_max': 22, 'unique_terms': 397, 'freq_moy': 1.6347607052896724}, '538': {'freq_max': 18, 'unique_terms': 123, 'freq_moy': 1.5040650406504066}, '539': {'freq_max': 19, 'unique_terms': 171, 'freq_moy': 1.5029239766081872}, '540': {'freq_max': 29, 'unique_terms': 285, 'freq_moy': 1.7298245614035088}, '541': {'freq_max': 56, 'unique_terms': 398, 'freq_moy': 1.8743718592964824}, '542': {'freq_max': 57, 'unique_terms': 399, 'freq_moy': 1.799498746867168}, '543': {'freq_max': 39, 'unique_terms': 410, 'freq_moy': 1.673170731707317}, '544': {'freq_max': 14, 'unique_terms': 168, 'freq_moy': 1.4523809523809523}, '545': {'freq_max': 85, 'unique_terms': 586, 'freq_moy': 1.9607508532423208}, '546': {'freq_max': 66, 'unique_terms': 572, 'freq_moy': 1.8601398601398602}, '548': {'freq_max': 28, 'unique_terms': 189, 'freq_moy': 1.507936507936508}, '549': {'freq_max': 32, 'unique_terms': 245, 'freq_moy': 1.6653061224489796}, '550': {'freq_max': 25, 'unique_terms': 275, 'freq_moy': 1.5454545454545454}, '551': {'freq_max': 41, 'unique_terms': 315, 'freq_moy': 1.7746031746031745}, '552': {'freq_max': 22, 'unique_terms': 320, 'freq_moy': 1.640625}, '553': {'freq_max': 19, 'unique_terms': 263, 'freq_moy': 1.6311787072243347}, '555': {'freq_max': 46, 'unique_terms': 284, 'freq_moy': 1.778169014084507}, '556': {'freq_max': 64, 'unique_terms': 442, 'freq_moy': 1.8936651583710407}, '557': {'freq_max': 20, 'unique_terms': 264, 'freq_moy': 1.6022727272727273}, '558': {'freq_max': 40, 'unique_terms': 356, 'freq_moy': 1.5449438202247192}, '559': {'freq_max': 13, 'unique_terms': 135, 'freq_moy': 1.474074074074074}, '560': {'freq_max': 16, 'unique_terms': 138, 'freq_moy': 1.6014492753623188}, '561': {'freq_max': 14, 'unique_terms': 153, 'freq_moy': 1.5751633986928104}, '562': {'freq_max': 23, 'unique_terms': 215, 'freq_moy': 1.572093023255814}, '563': {'freq_max': 7, 'unique_terms': 54, 'freq_moy': 1.3888888888888888}}\n"
     ]
    }
   ],
   "source": [
    "from Utils.Lab1 import *\n",
    "\n",
    "collection_TIME = loadData(\"./Data/Time/TIME.ALL\")\n",
    "pre_processed_collection_TIME = collection_lemmatize(remove_stop_words(tokenize_Regexp_corpus(collection_TIME),\"./Data/Time/TIME.STP\"))\n",
    "stats=get_stats_collection(pre_processed_collection_TIME)\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIf6vkTcNzSk"
   },
   "source": [
    "14- Ecrire une fonction `get_tf_normalise(term,doc_ID, index_frequence,stats_collection)` qui calcule la pondération fréquentielle du terme `term` pour le document d'identifiant `doc_ID` à partir de l'index de fréquence `index_frequence` et d'informations statistiques sur la collection stockée dans le dictionnaire `stats_collection` construit à l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KV4JLTONNzSl",
    "outputId": "26b0739d-1543-4c98-e721-1a2b347adc7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "def get_tf_normalise(term,doc_ID, index_frequence,stats_collection):\n",
    "        tf = get_tf(term,doc_ID, index_frequence)\n",
    "        tf_normalise = 0.5 + 0.5 * (tf /stats_collection[doc_ID][\"freq_max\"])\n",
    "        return tf_normalise\n",
    "\n",
    "print(get_tf_normalise(\"NASSAU\", '017',frequence_index,stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzTr7FYuNzSs"
   },
   "source": [
    "15- Ecrire une fonction `get_tf_logarithme_normalise(term,doc_ID, index_frequence,stats_collection)` qui calcule la pondération fréquentielle logarithmique normalisée du terme `term` pour le document d'identifiant `doc_ID` à partir de l'index de fréquence `index_frequence` et d'informations statistiques sur la collection stockées dans le dictionnaire `stats_collection` construit à l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_lGFB4XNzSt",
    "outputId": "d336bbab-2982-406b-a7a0-4f940b6f4593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3529759195388797\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "def get_tf_logarithme_normalise(term,doc_ID, index_frequence,stats_collection):\n",
    "        tf = get_tf(term,doc_ID, index_frequence)\n",
    "        tf_logarithme_normalise = (1 +log(tf))/(1 + log(stats_collection[doc_ID][\"freq_moy\"]))\n",
    "        return tf_logarithme_normalise\n",
    "\n",
    "print(get_tf_logarithme_normalise(\"NASSAU\", '017',frequence_index,stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4NiDe-ANzSy"
   },
   "source": [
    "16- Ecrire une fonction `get_idf (term ,index_frequence, nb_doc)` qui permet de calculer le logarithme de l'inverse normalisée de la fréquence documentaire $df_t$ d'un terme. `nb_doc`est le nombre de documents dans la collection. Il est égal à 523 pour la collection TIME ou vous pouvez le récupérer via le dictionnaire de statistiques construit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mS43H2ajNzSy",
    "outputId": "a1296236-6f05-4b61-b06b-66b41f99974b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.31367131500961"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "def get_idf(term,index_frequence,nb_doc):\n",
    "    return log(nb_doc/len(index_frequence[term].keys()))\n",
    "               \n",
    "get_idf(\"NASSAU\",frequence_index,523)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ramdo4XANzS0"
   },
   "source": [
    "**Traitement des requêtes**\n",
    "\n",
    "17- Ecrire une fonction `def processing_vectorial_query(query, inverted_index,stats_collection, weighting_scheme_document,weighting_scheme_query)` qui permet de renvoyer la liste de documents pertinents, ordonnés par pertinence, pour la requête `query` à partir de l'index de fréquence `inverted_index`. Les paramètres `weighting_scheme_document` et `weighting_scheme_query` permettent de définir le type de pondération choisie pour le document et pour la requête. On considérera les types suivants :\n",
    "\n",
    " + 'binary' : schema de pondération binaire.\n",
    " + 'frequency' : schema de pondération fréquentiel simple (`tf` seul).\n",
    " + 'tf_idf_normalize' : schema de pondération `tf_idf` avec pour la pondération fréquentielle normalisée pour le terme correspondant à la fréquence du terme dans le document.\n",
    " + 'tf_idf_logarithmic' : schema de pondération `tf_idf` avec pour la pondération fréquentielle logarithmique pour le terme correspondant à la fréquence du terme dans le document.\n",
    " + 'tf_idf_logarithmic_normalize' : schema de pondération `tf_idf` avec pour la pondération fréquentielle logarithmique normalisée pour le terme correspondant à la fréquence du terme dans le document.\n",
    "\n",
    "\n",
    "Vous pourrez pour cela vous appuyer sur l'algorithme décrit dans le support du cours 1 (version longue), slide 136. On ne considérera pour la requête que les schémas de pondération ('binary','frequency') et on prendra comme mesure de similarité le produit scalaire. On ne considérera donc pas de facteur de normalisation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MVz2HJlNzS0"
   },
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "def processing_vectorial_query(query, inverted_index, stats_collection, weighting_scheme_document,weighting_scheme_query):\n",
    "    relevant_docs = {}\n",
    "    counter_query= Counter()\n",
    "    query_pre_processed = pre_processed_query(query)\n",
    "    nb_doc = stats_collection[\"nb_docs\"]\n",
    "    norm_query=0.\n",
    "    for term in query_pre_processed:\n",
    "        w_term_query=0.\n",
    "        counter_query.update([term])\n",
    "        if weighting_scheme_query==\"binary\":\n",
    "            w_term_query = 1\n",
    "        if weighting_scheme_query==\"frequency\":\n",
    "            w_term_query = counter_query[term]\n",
    "        norm_query = norm_query + w_term_query*w_term_query\n",
    "        for doc in inverted_index[term]:\n",
    "            w_term_doc = 0.\n",
    "            relevant_docs[doc]=0.\n",
    "            if weighting_scheme_document==\"binary\":\n",
    "                w_term_doc=1\n",
    "            if weighting_scheme_document==\"frequency\":\n",
    "                w_term_doc = get_tf(term,doc,inverted_index)\n",
    "            if weighting_scheme_document==\"tf_idf_normalize\":\n",
    "                w_term_doc = get_tf_normalise(term,doc, inverted_index,stats_collection)*get_idf(term,inverted_index,nb_doc)\n",
    "            if weighting_scheme_document==\"tf_idf_logarithmic\":\n",
    "                w_term_doc = get_tf_logarithmique (term,doc, inverted_index)*get_idf(term,inverted_index,nb_doc)\n",
    "            if weighting_scheme_document==\"tf_idf_logarithmic_normalize\":\n",
    "                w_term_doc = get_tf_logarithme_normalise (term,doc, inverted_index,stats_collection)*get_idf(term,inverted_index,nb_doc)\n",
    "            relevant_docs[doc] = relevant_docs[doc] + w_term_doc*w_term_query\n",
    "    ordered_relevant_docs = OrderedDict(sorted(relevant_docs.items(), key=lambda t: t[1], reverse=True))\n",
    "    return ordered_relevant_docs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac4IUDRnNzS7"
   },
   "source": [
    "18- Tester votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYoa5UAANzS8",
    "outputId": "ddab9c5f-771d-4e4f-af38-490031c65b99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('498', 7.27136095608322),\n",
       "             ('480', 6.394875635876801),\n",
       "             ('434', 6.255619632969771),\n",
       "             ('418', 6.17671626631268),\n",
       "             ('396', 6.113117318058958),\n",
       "             ('390', 5.8214545742340995),\n",
       "             ('320', 5.711412817634378),\n",
       "             ('228', 5.409434587020661),\n",
       "             ('269', 5.08496650937611),\n",
       "             ('533', 4.971199491717411),\n",
       "             ('518', 4.966194865672923),\n",
       "             ('183', 4.342560162345133),\n",
       "             ('519', 4.294489458940759),\n",
       "             ('363', 4.2846632758551),\n",
       "             ('508', 4.246888239514111),\n",
       "             ('196', 4.092679424020515),\n",
       "             ('414', 4.044743999818459),\n",
       "             ('470', 4.002539431326653),\n",
       "             ('559', 3.5519677521233683),\n",
       "             ('351', 3.509092270162957),\n",
       "             ('317', 3.43441219031494),\n",
       "             ('019', 3.3794017378138346),\n",
       "             ('154', 3.2417725297211164),\n",
       "             ('017', 2.972791942954898),\n",
       "             ('221', 2.946938477256503),\n",
       "             ('260', 2.7037823994223804),\n",
       "             ('227', 2.6548586742737275),\n",
       "             ('524', 2.6131413287916656),\n",
       "             ('561', 2.5579822913233987),\n",
       "             ('367', 2.478621987544371),\n",
       "             ('045', 2.4660506562717743),\n",
       "             ('230', 2.4030573944165945),\n",
       "             ('464', 2.37173216637603),\n",
       "             ('106', 2.2292098278121597),\n",
       "             ('287', 2.1991416972602242),\n",
       "             ('163', 2.1689322108479105),\n",
       "             ('331', 2.163663646395846),\n",
       "             ('544', 2.1204988054999827),\n",
       "             ('043', 2.1041159037890744),\n",
       "             ('492', 2.0894923452406666),\n",
       "             ('537', 2.080416100870039),\n",
       "             ('552', 2.075433336315576),\n",
       "             ('501', 2.0364941253978857),\n",
       "             ('253', 2.0247208055248573),\n",
       "             ('302', 1.9840545220106678),\n",
       "             ('098', 1.9581166477422332),\n",
       "             ('048', 1.9340835607317401),\n",
       "             ('053', 1.930581307385437),\n",
       "             ('238', 1.9267755797839148),\n",
       "             ('529', 1.9203900746177658),\n",
       "             ('153', 1.9168745386702262),\n",
       "             ('384', 1.9136683788943956),\n",
       "             ('276', 1.9052184604992444),\n",
       "             ('193', 1.899401111935972),\n",
       "             ('256', 1.877972341612282),\n",
       "             ('504', 1.8607774689582341),\n",
       "             ('099', 1.8368554732188123),\n",
       "             ('225', 1.8008607673265802),\n",
       "             ('176', 1.7881703093749748),\n",
       "             ('461', 1.786317222342698),\n",
       "             ('558', 1.7776214311036826),\n",
       "             ('562', 1.777487119336094),\n",
       "             ('516', 1.7656375122310028),\n",
       "             ('442', 1.754969609945401),\n",
       "             ('223', 1.751766279250586),\n",
       "             ('236', 1.7413104703684779),\n",
       "             ('545', 1.740172191950658),\n",
       "             ('426', 1.7393744084914466),\n",
       "             ('092', 1.73680129499613),\n",
       "             ('186', 1.7330549358636895),\n",
       "             ('335', 1.7285392165404763),\n",
       "             ('536', 1.714550740640101),\n",
       "             ('299', 1.7097057178162256),\n",
       "             ('543', 1.704364818947985),\n",
       "             ('346', 1.687983860390159),\n",
       "             ('250', 1.6631287866597635),\n",
       "             ('308', 1.661846553099929),\n",
       "             ('443', 1.6554210641243432),\n",
       "             ('334', 1.6482290984980748),\n",
       "             ('459', 1.644263968280074),\n",
       "             ('058', 1.637203761999809),\n",
       "             ('247', 1.6351173134540333),\n",
       "             ('188', 1.6303039112223237),\n",
       "             ('386', 1.615515796277963),\n",
       "             ('341', 1.60906414406858),\n",
       "             ('070', 1.5957185639200304),\n",
       "             ('431', 1.5955206835181082),\n",
       "             ('345', 1.5945818984981308),\n",
       "             ('062', 1.580305894431161),\n",
       "             ('104', 1.5724271390603597),\n",
       "             ('330', 1.5663407755828977),\n",
       "             ('405', 1.5645280782692572),\n",
       "             ('293', 1.5595367818286219),\n",
       "             ('040', 1.5595172197483962),\n",
       "             ('369', 1.5593692205566434),\n",
       "             ('556', 1.5568157026777827),\n",
       "             ('215', 1.5560808110828663),\n",
       "             ('024', 1.5542755640416046),\n",
       "             ('243', 1.5194659699932307),\n",
       "             ('057', 1.4855739308310905),\n",
       "             ('365', 1.4623203134952376),\n",
       "             ('138', 1.4598208126988004),\n",
       "             ('217', 1.4430666834957813),\n",
       "             ('463', 1.4327095419412044),\n",
       "             ('353', 1.4186696876220701),\n",
       "             ('021', 1.412564317485112),\n",
       "             ('460', 1.4004909911036378),\n",
       "             ('067', 1.3953443179250573),\n",
       "             ('028', 1.3920053455607566),\n",
       "             ('252', 1.3633831223221113),\n",
       "             ('337', 1.3628119597767494),\n",
       "             ('204', 1.3524606057146018),\n",
       "             ('348', 1.3410902181835118),\n",
       "             ('126', 1.3169220304333085),\n",
       "             ('392', 1.3147456403695323),\n",
       "             ('105', 1.3091487357602303),\n",
       "             ('029', 1.3005735422794098),\n",
       "             ('472', 1.2950951976644287),\n",
       "             ('380', 1.2913691476619256),\n",
       "             ('071', 1.2498273658691283),\n",
       "             ('370', 1.1604666891115925)])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_vectorial_query(\"KENNEDY KENNEDY ADMINISTRATION PRESSURE ON NGO DINH DIEM TO STOP\", frequence_index, stats,\"tf_idf_logarithmic_normalize\",\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD9k8Si5NzS_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "S5PsGu_cNzQ9"
   ],
   "name": "Lab2_ModelesDeRecherche-Student-final-Correction-all.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
